# tk_app-ai.py  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# tk_app-ai.py â”€â”€ Gemini-Flash Ã— Dual-Encoderã€€Response-Chooser â”€â”€
"""
â€£ è«‹å…ˆï¼špip install google-generativeai transformers safetensors torch
â€£ æ”¾ç½®ï¼š
      model.safetensors
      tokenizer.json / vocab.txt / config.json
â€£ åŸ·è¡Œï¼špython tk_app.py
"""
import os, threading
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
from pathlib import Path
import torch
from safetensors.torch import load_file
from transformers import AutoTokenizer
from google.generativeai import GenerativeModel, configure
from dual_encoder import DualTowerPairClassifier   # â¬… ä½ å°ˆæ¡ˆè£¡çš„é¡åˆ¥

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è·¯å¾‘ & è£ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT          = Path(__file__).parent
WEIGHT_FILE   = ROOT / "model.safetensors"
TOKENIZER_DIR = ROOT
DEVICE        = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¼‰å…¥ tokenizer / Dual-Encoder (ä¸€æ¬¡) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)

model = DualTowerPairClassifier(base_model="distilbert-base-uncased", include_prompt=True)   # è‹¥éœ€é—œé–‰promptè«‹æ”¹ç‚ºFalse
state = load_file(WEIGHT_FILE, device="cpu")     # â† å›ºå®šè¼‰åˆ° CPU
model.load_state_dict(state, strict=False)
model.to(DEVICE)                                # å†æ¬å» GPU/CPU åŸ·è¡Œè£ç½®
model.eval()

@torch.inference_mode()
def predict(prompt: str, resp_a: str, resp_b: str) -> int:
    """
    å›å‚³ï¼š0 âœ A è¼ƒä½³ã€1 âœ B è¼ƒä½³ã€2 âœ Tie
    â”€ æ¯æ®µå„è‡ª token izeï¼Œå¯é¿å…ç¶­åº¦å‡ºéŒ¯ï¼Œä¹Ÿè¼ƒå¥½è£åˆ‡é•·åº¦ã€‚"""
    tok_p = tokenizer(prompt,  truncation=True, padding="max_length",
                      max_length=512, return_tensors="pt").to(DEVICE)
    tok_a = tokenizer(resp_a,  truncation=True, padding="max_length",
                      max_length=512, return_tensors="pt").to(DEVICE)
    tok_b = tokenizer(resp_b,  truncation=True, padding="max_length",
                      max_length=512, return_tensors="pt").to(DEVICE)

    out = model(
        p_input_ids      = tok_p["input_ids"],
        p_attention_mask = tok_p["attention_mask"],
        a_input_ids      = tok_a["input_ids"],
        a_attention_mask = tok_a["attention_mask"],
        b_input_ids      = tok_b["input_ids"],
        b_attention_mask = tok_b["attention_mask"],
    )
    return out["logits"].argmax(-1).item()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI ä½ˆå±€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
root = tk.Tk()
root.title("Gemini â†” Dual-Encoder  Response Chooser")
root.configure(bg="#1e1e1e")

style = ttk.Style(root)
style.theme_use("clam")
style.configure(".", background="#1e1e1e", foreground="#d4d4d4")
style.configure("TButton", background="#0e639c", foreground="white")
style.map("TButton", background=[("active", "#1177bb")])

def add_lbl(text, r, c, cs=1):
    ttk.Label(root, text=text).grid(row=r, column=c, columnspan=cs,
                                    sticky="w", padx=4, pady=2)

add_lbl("Gemini API-Key:", 0,0)
api_entry = ttk.Entry(root, width=50, show="*"); api_entry.grid(row=0, column=1, columnspan=3, sticky="we", padx=4)

add_lbl("Prompt:", 1,0)
prompt_box = scrolledtext.ScrolledText(root, height=4, width=80, bg="#252526", fg="#d4d4d4")
prompt_box.grid(row=1, column=1, columnspan=3, padx=4, pady=2)

add_lbl("Response A  (gemini-2.5-flash):", 2,0)
respA_box = scrolledtext.ScrolledText(root, height=6, width=80, bg="#252526", fg="#d4d4d4")
respA_box.grid(row=2, column=1, columnspan=3, padx=4, pady=2)

add_lbl("Response B  (gemini-2.0-flash):", 3,0)
respB_box = scrolledtext.ScrolledText(root, height=6, width=80, bg="#252526", fg="#d4d4d4")
respB_box.grid(row=3, column=1, columnspan=3, padx=4, pady=2)

status_var = tk.StringVar(value="âœ  è¼¸å…¥ Prompt èˆ‡ API-Keyï¼Œç„¶å¾ŒæŒ‰ã€Œç”¢ç”Ÿå›æ‡‰ã€")
ttk.Label(root, textvariable=status_var, foreground="#c586c0").grid(
    row=4, column=0, columnspan=4, sticky="w", padx=4, pady=4)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gemini ç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_responses():
    key    = api_entry.get().strip()
    prompt = prompt_box.get("1.0", tk.END).strip()
    if not key or not prompt:
        messagebox.showwarning("ç¼ºåƒæ•¸", "è«‹å¡« API-Key èˆ‡ Prompt"); return

    status_var.set("ğŸ”„ Gemini ç”Ÿæˆä¸­â€¦")
    respA_box.delete("1.0", tk.END); respB_box.delete("1.0", tk.END)

    def _worker():
        try:
            configure(api_key=key)
            mdlA = GenerativeModel("gemini-2.5-flash")
            mdlB = GenerativeModel("gemini-2.0-flash")

            rA = mdlA.generate_content(prompt).text
            rB = mdlB.generate_content(prompt).text

            root.after(0, lambda: respA_box.insert(tk.END, rA))
            root.after(0, lambda: respB_box.insert(tk.END, rB))
            root.after(0, lambda: status_var.set("âœ“  è«‹åœ¨å¿ƒè£¡é¸è¼ƒä½³å›æ‡‰ï¼Œå†æŒ‰ã€Œæ¨¡å‹åˆ¤æ–·ã€"))
        except Exception as e:
            root.after(0, lambda: status_var.set(f"âš ï¸  ç”Ÿæˆå¤±æ•—: {e}"))
    threading.Thread(target=_worker, daemon=True).start()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dual-Encoder æ¨ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_model():
    prompt = prompt_box.get("1.0", tk.END).strip()
    a      = respA_box.get("1.0", tk.END).strip()
    b      = respB_box.get("1.0", tk.END).strip()
    if not (prompt and a and b):
        messagebox.showwarning("ç¼ºå›æ‡‰", "è«‹å…ˆç”¢ç”Ÿå›æ‡‰"); return
    status_var.set("ğŸ¤– æ¨¡å‹æ¨ç†ä¸­â€¦")

    def _work():
        try:
            pred = predict(prompt, a, b)
            msg  = ["æ¨¡å‹åˆ¤å®šï¼šA è¼ƒä½³ ğŸ†",
                    "æ¨¡å‹åˆ¤å®šï¼šB è¼ƒä½³ ğŸ†",
                    "æ¨¡å‹åˆ¤å®šï¼šæ‰“å¹³ ğŸ¤"][pred]
            root.after(0, lambda: status_var.set(msg))
        except Exception as e:
            root.after(0, lambda: status_var.set(f"âš ï¸ æ¨ç†å¤±æ•—: {e}"))
    threading.Thread(target=_work, daemon=True).start()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æŒ‰éˆ• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
gen_btn  = ttk.Button(root, text="ç”¢ç”Ÿå›æ‡‰", command=generate_responses)
pred_btn = ttk.Button(root, text="æ¨¡å‹åˆ¤æ–·", command=run_model)
gen_btn.grid(row=5, column=1, sticky="we", pady=6)
pred_btn.grid(row=5, column=2, sticky="we", pady=6)

root.columnconfigure(1, weight=1)   # è®“è¼¸å…¥æ¡†éš¨è¦–çª—æ”¾å¤§
root.mainloop()

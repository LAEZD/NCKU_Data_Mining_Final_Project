# LLM Classification Fine-Tuning - UNIFIED & MODULAR PIPELINE
# ===============================================================
# ‰ΩúËÄÖÔºö[‰Ω†ÁöÑÂêçÂ≠ó/ÂúòÈöä]
# ÁâàÊú¨Ôºö1.0
# ÊèèËø∞Ôºö
# ‰∏ÄÂÄãÁµ±‰∏ÄÁöÑ„ÄÅÊ®°ÁµÑÂåñÁöÑÊµÅÊ∞¥Á∑öÔºåÂèØÂú®Êú¨Âú∞ÔºàÊúâÁ∂≤Ë∑ØÔºâÂíå KaggleÔºàÂÆåÂÖ®Èõ¢Á∑öÔºâÁí∞Â¢É‰∏≠ÁÑ°Á∏´ÂàáÊèõ„ÄÇ
# - Ëá™ÂãïÂÅµÊ∏¨Áí∞Â¢ÉÔºàÊú¨Âú∞ vs KaggleÔºâ„ÄÇ
# - ‰∏≠ÂøÉÂåñÈÖçÁΩÆÁÆ°ÁêÜÔºàConfig È°ûÔºâ„ÄÇ
# - Ê®°ÁµÑÂåñÊµÅÁ®ãÔºàPipelineModules È°ûÔºâÔºåÊñπ‰æøÂçî‰ΩúËàáÁ∂≠Ë≠∑„ÄÇ
# ===============================================================

import os
import warnings
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    Trainer, 
    TrainingArguments,
    EarlyStoppingCallback
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss
from sklearn.utils.class_weight import compute_class_weight

# ÂøΩÁï•‰∏çÂøÖË¶ÅÁöÑË≠¶Âëä
warnings.filterwarnings('ignore')

# --------------------------------------------------------------------------
# 1. ‰∏≠ÂøÉÂåñÈÖçÁΩÆ (Centralized Configuration)
# --------------------------------------------------------------------------
class Config:
    """
    Áµ±‰∏ÄÁÆ°ÁêÜÊâÄÊúâË®≠ÂÆö„ÄÅË∑ØÂæëÂíåË∂ÖÂèÉÊï∏„ÄÇ
    Ëá™ÂãïÂÅµÊ∏¨Áí∞Â¢É‰∏¶Ë®≠ÂÆöÂ∞çÊáâÁöÑË∑ØÂæë„ÄÇ
    """
    def __init__(self):
        # --- Âü∫Á§éË®≠ÂÆö ---
        self.MODEL_NAME = 'distilbert-base-uncased'
        self.QUICK_TEST = False  # Ë®≠ÁÇ∫ True ÂèØÁî®Â∞ëÈáèË≥áÊñôÂø´ÈÄüÊ∏¨Ë©¶
        self.QUICK_TEST_SIZE = 2000
        self.RANDOM_STATE = 42
        
        # --- Ë®ìÁ∑¥Ë∂ÖÂèÉÊï∏ ---
        self.EPOCHS = 4
        self.LEARNING_RATE = 1e-5
        self.TRAIN_BATCH_SIZE = 8
        self.EVAL_BATCH_SIZE = 8
        self.WEIGHT_DECAY = 0.03
        self.WARMUP_STEPS = 600
        self.LOGGING_STEPS = 50
        self.EVAL_STEPS = 600
        self.SAVE_STEPS = 600
        self.SAVE_TOTAL_LIMIT = 2
        self.LABEL_SMOOTHING = 0.1
        self.VALIDATION_SIZE = 0.15

        # --- Áí∞Â¢ÉÂÅµÊ∏¨ËàáË∑ØÂæëË®≠ÂÆö ---
        self.IS_KAGGLE = os.path.exists('/kaggle/input')
        
        if self.IS_KAGGLE:
            print("üöÄ Running in Kaggle environment (Offline Mode)")
            # Kaggle ÁöÑË≥áÊñôËº∏ÂÖ•Ë∑ØÂæë
            self.DATA_DIR = "/kaggle/input/llm-classification-finetuning"
            self.TRAIN_PATH = os.path.join(self.DATA_DIR, "train.csv")
            self.TEST_PATH = os.path.join(self.DATA_DIR, "test.csv")
            
            # Kaggle ÁöÑÊ®°ÂûãËº∏ÂÖ•Ë∑ØÂæëÔºàÂÅáË®≠Ê®°ÂûãÂ∑≤‰∏äÂÇ≥ÁÇ∫Êï∏ÊìöÈõÜÔºâ
            self.KAGGLE_MODEL_PATH = "/kaggle/input/distilbert_model/transformers/default/1/distilbert_model"
            
            # Kaggle ÁöÑËº∏Âá∫Ë∑ØÂæë
            self.OUTPUT_DIR = "/kaggle/working/results"
            self.LOGGING_DIR = "/kaggle/working/logs"
            self.SUBMISSION_PATH = "/kaggle/working/submission.csv"
            self.VALIDATION_RESULTS_PATH = "/kaggle/working/validation_results.csv"
        else:
            print("üè† Running in Local environment (Online Mode)")
            # Êú¨Âú∞ÁöÑË≥áÊñôË∑ØÂæë
            self.DATA_DIR = "."
            self.TRAIN_PATH = os.path.join(self.DATA_DIR, "train.csv")
            self.TEST_PATH = os.path.join(self.DATA_DIR, "test.csv")
            
            # Êú¨Âú∞ÁöÑÊ®°ÂûãË∑ØÂæëÔºàÊúÉÂæû Hugging Face ‰∏ãËºâÔºâ
            self.KAGGLE_MODEL_PATH = None # Êú¨Âú∞Áî®‰∏çÂà∞
            
            # Êú¨Âú∞ÁöÑËº∏Âá∫Ë∑ØÂæë
            self.OUTPUT_DIR = "./results"
            self.LOGGING_DIR = "./logs"
            self.SUBMISSION_PATH = os.path.join(self.OUTPUT_DIR, "submission.csv")
            self.VALIDATION_RESULTS_PATH = os.path.join(self.OUTPUT_DIR, "validation_results.csv")
            
            # Á¢∫‰øùÊú¨Âú∞Ëº∏Âá∫ÁõÆÈåÑÂ≠òÂú®
            os.makedirs(self.OUTPUT_DIR, exist_ok=True)
            os.makedirs(self.LOGGING_DIR, exist_ok=True)
            
        # --- Ë®≠ÂÇôË®≠ÂÆö ---
        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üíª Using device: {self.DEVICE}")

# --------------------------------------------------------------------------
# 2. Ê®°ÁµÑÂåñÊµÅÊ∞¥Á∑ö (Modular Pipeline)
# --------------------------------------------------------------------------
class PipelineModules:
    """
    Â∞ÅË£ùÊµÅÊ∞¥Á∑ö‰∏≠ÁöÑÊ†∏ÂøÉÂäüËÉΩÊ®°ÁµÑ„ÄÇ
    ÊâÄÊúâÊñπÊ≥ïÈÉΩÊòØÈùúÊÖãÁöÑÔºåÊñπ‰æøÁõ¥Êé•Ë™øÁî®„ÄÇ
    """
    
    @staticmethod
    def load_and_preprocess_data(config: Config):
        """ËºâÂÖ•Ë®ìÁ∑¥ÂíåÊ∏¨Ë©¶Ë≥áÊñôÔºå‰∏¶ÈÄ≤Ë°åÈ†êËôïÁêÜ„ÄÇ"""
        print("\n[Module 1/5] üìä Loading and preprocessing data...")
        
        # ËºâÂÖ•Ë®ìÁ∑¥Ë≥áÊñô
        try:
            df = pd.read_csv(config.TRAIN_PATH)
            print(f"  - Training data loaded: {df.shape}")
        except FileNotFoundError:
            print(f"‚ùå ERROR: Training file not found at {config.TRAIN_PATH}")
            raise
            
        # Âø´ÈÄüÊ∏¨Ë©¶Ê®°Âºè
        if config.QUICK_TEST and len(df) > config.QUICK_TEST_SIZE:
            df = df.sample(n=config.QUICK_TEST_SIZE, random_state=config.RANDOM_STATE).reset_index(drop=True)
            print(f"  - Quick test mode enabled, sampled to {df.shape}")

        # ÂâµÂª∫Ê®ôÁ±§
        def get_label(row):
            if row["winner_model_a"] == 1: return 0
            if row["winner_model_b"] == 1: return 1
            return 2
        df["label"] = df.apply(get_label, axis=1)
        
        # ÊñáÊú¨È†êËôïÁêÜ
        def create_optimized_input(row):
            prompt = str(row['prompt']).strip()
            response_a = str(row['response_a']).strip()
            response_b = str(row['response_b']).strip()
            
            len_a, len_b = len(response_a), len(response_b)
            text = f"Compare responses to: {prompt} [SEP] Option A ({len_a} chars): {response_a} [SEP] Option B ({len_b} chars): {response_b}"
            
            if len(text) > 480:
                max_prompt = min(100, len(prompt))
                max_resp = min(140, len(response_a), len(response_b))
                text = f"Compare: {prompt[:max_prompt]} [SEP] A: {response_a[:max_resp]} [SEP] B: {response_b[:max_resp]}"
            return text

        df["text"] = df.apply(create_optimized_input, axis=1)
        print("  - Text inputs and labels processed.")
        
        # ËºâÂÖ•Ê∏¨Ë©¶Ë≥áÊñô
        df_test = None
        if os.path.exists(config.TEST_PATH):
            file_size_mb = os.path.getsize(config.TEST_PATH) / (1024**2)
            if file_size_mb > 100:
                print(f"  - Large test file ({file_size_mb:.1f} MB) detected. Will process in batches.")
            else:
                df_test = pd.read_csv(config.TEST_PATH)
                df_test["text"] = df_test.apply(create_optimized_input, axis=1)
                print(f"  - Test data loaded and processed: {df_test.shape}")
        else:
            print("  - WARNING: Test file not found. Inference will be skipped.")
            
        return df, df_test

    @staticmethod
    def load_model_and_tokenizer(config: Config):
        """Ê†πÊìöÁí∞Â¢ÉËºâÂÖ•Ê®°ÂûãÂíåÂàÜË©ûÂô®„ÄÇ"""
        print("\n[Module 2/5] ü§ñ Loading model and tokenizer...")
        
        try:
            if config.IS_KAGGLE:
                # Kaggle Èõ¢Á∑öÊ®°Âºè
                model_path = config.KAGGLE_MODEL_PATH
                if not os.path.exists(model_path):
                    raise FileNotFoundError(f"Offline model not found at {model_path}")
                
                print(f"  - Loading OFFLINE from: {model_path}")
                tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
                model = AutoModelForSequenceClassification.from_pretrained(
                    model_path, num_labels=3, local_files_only=True
                )
            else:
                # Êú¨Âú∞Á∑ö‰∏äÊ®°Âºè
                model_path = config.MODEL_NAME
                print(f"  - Loading ONLINE from Hugging Face: {model_path}")
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                model = AutoModelForSequenceClassification.from_pretrained(
                    model_path, num_labels=3
                )
            
            # Ê∑ªÂä† padding token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model.to(config.DEVICE)
            print(f"  - ‚úÖ Model and tokenizer loaded successfully.")
            return tokenizer, model, model_path

        except Exception as e:
            print(f"‚ùå FATAL ERROR in model loading: {e}")
            raise
    
    @staticmethod
    def create_datasets(df, tokenizer, config: Config):
        """ÂâµÂª∫Ë®ìÁ∑¥ÂíåÈ©óË≠âË≥áÊñôÈõÜ„ÄÇ"""
        print("\n[Module 3/5] üìö Creating datasets...")
        
        train_indices, val_indices, train_labels, val_labels = train_test_split(
            df.index, df["label"].tolist(), 
            test_size=config.VALIDATION_SIZE, 
            random_state=config.RANDOM_STATE, 
            stratify=df["label"]
        )
        
        train_texts = df.loc[train_indices, "text"].tolist()
        val_texts = df.loc[val_indices, "text"].tolist()
        
        print(f"  - Training samples: {len(train_texts)}")
        print(f"  - Validation samples: {len(val_texts)}")

        class LLMDataset(Dataset):
            def __init__(self, texts, labels=None):
                self.encodings = tokenizer(texts, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
                self.labels = labels
            def __len__(self):
                return len(self.encodings["input_ids"])
            def __getitem__(self, idx):
                item = {key: val[idx] for key, val in self.encodings.items()}
                if self.labels is not None:
                    item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
                return item

        train_dataset = LLMDataset(train_texts, train_labels)
        val_dataset = LLMDataset(val_texts, val_labels)
        
        # Ë®àÁÆóÈ°ûÂà•Ê¨äÈáç
        class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
        class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}
        print(f"  - Class weights calculated: {class_weights_dict}")

        return train_dataset, val_dataset, val_labels, val_indices, class_weights_dict

    @staticmethod
    def setup_trainer(model, train_dataset, val_dataset, class_weights_dict, config: Config):
        """Ë®≠ÂÆö‰∏¶ËøîÂõû Trainer Áâ©‰ª∂„ÄÇ"""
        print("\n[Module 4/5] üõ†Ô∏è Setting up trainer...")

        training_args = TrainingArguments(
            output_dir=config.OUTPUT_DIR,
            num_train_epochs=config.EPOCHS,
            learning_rate=config.LEARNING_RATE,
            per_device_train_batch_size=config.TRAIN_BATCH_SIZE,
            per_device_eval_batch_size=config.EVAL_BATCH_SIZE,
            weight_decay=config.WEIGHT_DECAY,
            warmup_steps=config.WARMUP_STEPS,
            logging_dir=config.LOGGING_DIR,
            logging_steps=config.LOGGING_STEPS,
            eval_strategy="steps",
            eval_steps=config.EVAL_STEPS,
            save_strategy="steps",
            save_steps=config.SAVE_STEPS,
            load_best_model_at_end=True,
            metric_for_best_model="eval_log_loss",
            greater_is_better=False,
            save_total_limit=config.SAVE_TOTAL_LIMIT,
            fp16=torch.cuda.is_available(), # Â¶ÇÊûúÊúâCUDAÔºåÂâáÂïüÁî®FP16
            dataloader_pin_memory=False,
            report_to="none",
            seed=config.RANDOM_STATE,
            lr_scheduler_type="linear",
        )
        
        def compute_metrics(eval_pred):
            logits, labels = eval_pred
            probabilities = torch.nn.functional.softmax(torch.from_numpy(logits), dim=-1).numpy()
            preds = np.argmax(logits, axis=-1)
            
            accuracy = accuracy_score(labels, preds)
            logloss = log_loss(labels, probabilities, eps=1e-7)
            
            return {"accuracy": accuracy, "log_loss": logloss}
        
        # ÈáçÊñ∞ÂàùÂßãÂåñÂàÜÈ°ûÂ±§Ê¨äÈáç
        if hasattr(model, 'classifier'):
            print("  - Reinitializing classifier weights...")
            model.classifier.apply(lambda module: module.reset_parameters() if hasattr(module, 'reset_parameters') else None)
        
        class WeightedTrainer(Trainer):
            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
                labels = inputs.pop("labels")
                outputs = model(**inputs)
                logits = outputs.logits
                
                weights = torch.tensor(list(class_weights_dict.values()), device=logits.device, dtype=torch.float)
                loss_fct = torch.nn.CrossEntropyLoss(weight=weights, label_smoothing=config.LABEL_SMOOTHING)
                loss = loss_fct(logits, labels)
                
                return (loss, outputs) if return_outputs else loss

        trainer = WeightedTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        
        print("  - Trainer setup complete.")
        return trainer

    @staticmethod
    def run_inference_and_save(trainer, df_test, tokenizer, config: Config):
        """Âü∑Ë°åÊ∏¨Ë©¶ÈõÜÊé®ÁêÜ‰∏¶ÂÑ≤Â≠òÊèê‰∫§Ê™îÊ°à„ÄÇ"""
        print("\n[Module 5/5] üîÆ Running inference and creating submission...")
        
        if df_test is None:
            if not os.path.exists(config.TEST_PATH):
                print("  - Test file not found, skipping inference.")
                # Âú® Kaggle Áí∞Â¢É‰∏≠ÔºåÂ¶ÇÊûúÊ≤íÊúâ test.csvÔºåÈÄöÂ∏∏ÊÑèÂë≥ËëóÈÄôÊòØÂÄã Code Competition ÁöÑÁ¨¨‰∫åÈöéÊÆµ
                # ÂâµÂª∫‰∏ÄÂÄãÂÅáÁöÑ submission.csv ÈÅøÂÖçÊèê‰∫§ÈåØË™§
                if config.IS_KAGGLE:
                     pd.DataFrame({'id': [], 'winner_model_a': [], 'winner_model_b': [], 'winner_tie': []}).to_csv(config.SUBMISSION_PATH, index=False)
                     print("  - Empty submission.csv created for Kaggle environment.")
                return

        def process_test_batch(test_df_chunk, tokenizer):
            """ËôïÁêÜÂñÆ‰∏ÄÊâπÊ¨°ÁöÑÊ∏¨Ë©¶Ë≥áÊñô"""
            class TestDataset(Dataset):
                def __init__(self, texts):
                    self.encodings = tokenizer(texts, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
                def __len__(self): return len(self.encodings["input_ids"])
                def __getitem__(self, idx): return {key: val[idx] for key, val in self.encodings.items()}
            
            test_dataset = TestDataset(test_df_chunk["text"].tolist())
            preds = trainer.predict(test_dataset)
            probs = torch.nn.functional.softmax(torch.from_numpy(preds.predictions), dim=-1).numpy()
            return probs

        try:
            # Ê†πÊìöÊ™îÊ°àÂ§ßÂ∞èÊ±∫ÂÆöÊòØÂê¶ÂàÜÊâπËôïÁêÜ
            file_size_mb = os.path.getsize(config.TEST_PATH) / (1024**2) if os.path.exists(config.TEST_PATH) else 0
            
            if file_size_mb > 100:
                print(f"  - Processing large test file in batches...")
                all_ids, all_probs = [], []
                chunk_iter = pd.read_csv(config.TEST_PATH, chunksize=1000)
                
                for i, chunk in enumerate(chunk_iter):
                    print(f"    - Processing batch {i+1}...")
                    chunk["text"] = chunk.apply(PipelineModules.load_and_preprocess_data.create_optimized_input, axis=1)
                    probs = process_test_batch(chunk, tokenizer)
                    all_ids.extend(chunk["id"].tolist())
                    all_probs.append(probs)
                    torch.cuda.empty_cache()
                
                probabilities = np.vstack(all_probs)
                test_ids = all_ids
            else:
                print("  - Processing test file in a single batch...")
                test_ids = df_test["id"].tolist()
                probabilities = process_test_batch(df_test, tokenizer)
            
            submission = pd.DataFrame({"id": test_ids})
            submission["winner_model_a"] = probabilities[:, 0]
            submission["winner_model_b"] = probabilities[:, 1]
            submission["winner_tie"] = probabilities[:, 2]

            # Á¢∫‰øùÊ¶ÇÁéáÁ∏ΩÂíåÁÇ∫ 1
            row_sums = submission[["winner_model_a", "winner_model_b", "winner_tie"]].sum(axis=1)
            submission["winner_model_a"] /= row_sums
            submission["winner_model_b"] /= row_sums
            submission["winner_tie"] /= row_sums

            submission.to_csv(config.SUBMISSION_PATH, index=False)
            print(f"  - ‚úÖ Submission file saved to: {config.SUBMISSION_PATH}")
            
            # È°ØÁ§∫È†êÊ∏¨ÂàÜ‰Ωà
            final_preds = np.argmax(probabilities, axis=-1)
            print("\n  - Test prediction distribution:")
            print(f"    Model A wins: {np.sum(final_preds == 0)}")
            print(f"    Model B wins: {np.sum(final_preds == 1)}")  
            print(f"    Ties:         {np.sum(final_preds == 2)}")

        except Exception as e:
            print(f"‚ùå ERROR during test inference: {e}")
            print("  - Creating a dummy submission file to prevent failure.")
            dummy_sub = pd.DataFrame({"id": [0], "winner_model_a": [0.33], "winner_model_b": [0.33], "winner_tie": [0.34]})
            dummy_sub.to_csv(config.SUBMISSION_PATH, index=False)

# --------------------------------------------------------------------------
# 3. ‰∏ªÂü∑Ë°åÊµÅÁ®ã (Main Execution Flow)
# --------------------------------------------------------------------------
def main():
    """
    Âü∑Ë°åÂÆåÊï¥ÁöÑÊ©üÂô®Â≠∏ÁøíÊµÅÊ∞¥Á∑ö„ÄÇ
    """
    # === Ê≠•È©ü 1: ÂàùÂßãÂåñÈÖçÁΩÆ ===
    config = Config()

    # === Ê≠•È©ü 2: ËºâÂÖ•‰∏¶È†êËôïÁêÜË≥áÊñô ===
    df, df_test = PipelineModules.load_and_preprocess_data(config)

    # === Ê≠•È©ü 3: ËºâÂÖ•Ê®°ÂûãÂíåÂàÜË©ûÂô® ===
    tokenizer, model, model_path_info = PipelineModules.load_model_and_tokenizer(config)

    # === Ê≠•È©ü 4: ÂâµÂª∫Ë≥áÊñôÈõÜ ===
    train_dataset, val_dataset, val_labels, val_indices, class_weights = PipelineModules.create_datasets(
        df, tokenizer, config
    )

    # === Ê≠•È©ü 5: Ë®≠ÂÆö‰∏¶Âü∑Ë°åË®ìÁ∑¥ ===
    trainer = PipelineModules.setup_trainer(model, train_dataset, val_dataset, class_weights, config)
    
    print("\nüöÄ Starting training...")
    trainer.train()
    print("‚úÖ Training complete.")

    # === Ê≠•È©ü 6: ÊúÄÁµÇÈ©óË≠âËàáË©ï‰º∞ ===
    print("\nüìà Final validation...")
    val_preds = trainer.predict(val_dataset)
    val_probs = torch.nn.functional.softmax(torch.from_numpy(val_preds.predictions), dim=-1).numpy()
    val_final_loss = log_loss(val_labels, val_probs, eps=1e-7)
    val_final_acc = accuracy_score(val_labels, np.argmax(val_probs, axis=1))

    print(f"\n{'='*60}")
    print(f"üèÜ FINAL VALIDATION RESULTS")
    print(f"  - Model Used: {model_path_info}")
    print(f"  - Log Loss:   {val_final_loss:.6f}")
    print(f"  - Accuracy:   {val_final_acc:.4f}")
    print(f"{'='*60}\n")
    
    # ÂÑ≤Â≠òÈ©óË≠âÁµêÊûú‰ª•‰æõÂàÜÊûê
    val_results = pd.DataFrame({
        "id": df.loc[val_indices, "id"],
        "gt_label": val_labels,
        "pred_prob_a": val_probs[:, 0],
        "pred_prob_b": val_probs[:, 1],
        "pred_prob_tie": val_probs[:, 2]
    })
    val_results.to_csv(config.VALIDATION_RESULTS_PATH, index=False)
    print(f"  - Validation results saved to: {config.VALIDATION_RESULTS_PATH}")

    # === Ê≠•È©ü 7: Ê∏¨Ë©¶ÈõÜÊé®ÁêÜËàáÊèê‰∫§ ===
    PipelineModules.run_inference_and_save(trainer, df_test, tokenizer, config)

    print("\nüéâ Pipeline finished successfully!")

if __name__ == "__main__":
    main()
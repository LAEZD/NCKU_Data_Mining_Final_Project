# Kaggle Offline Inference Script - kaggle_last.py
# ==================================================
# å°ˆé–€ç”¨æ–¼ Kaggle é›¢ç·šç’°å¢ƒçš„æ¨ç†è…³æœ¬
# ä½¿ç”¨é è¨“ç·´çš„æ¨¡å‹é€²è¡Œé æ¸¬ï¼Œç„¡éœ€é‡æ–°è¨“ç·´

import os
import json
import numpy as np
import pandas as pd
import torch
import hashlib
import re
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel
from typing import Dict, Any, List, Optional, Union
from tqdm import tqdm

# >>>>>>> ADD >>>>>>>
# ä½¿ç”¨èˆ‡è¨“ç·´éšæ®µå®Œå…¨ç›¸åŒçš„ MetadataFeatures ä¾†é¿å…ç‰¹å¾µè¨ˆç®—èˆ‡é †åºä¸ä¸€è‡´
# Metadata Features Extraction Module
# ===================================
# æå–å’Œè¨ˆç®—å››å€‹æ ¸å¿ƒå…ƒæ•¸æ“šç‰¹å¾µ
# é€™äº›ç‰¹å¾µå…·æœ‰å¼·ä¿¡è™Ÿä¸”èƒ½æœ‰æ•ˆå€åˆ†å›ç­”è³ªé‡

import os
import hashlib # <--- æ–°å¢å°å…¥
import pandas as pd
import numpy as np
import re
from typing import Dict, List, Tuple
from tqdm import tqdm 
tqdm.pandas(desc="Extracting metadata features") 

CACHE_DIR = "./cache/metadata" # <--- æ–°å¢å¿«å–ç›®éŒ„å®šç¾©

class MetadataFeatures:
    """
    å…ƒæ•¸æ“šç‰¹å¾µæå–é¡ï¼Œæä¾›å››å€‹æ ¸å¿ƒç‰¹å¾µçš„è¨ˆç®—æ–¹æ³•
    """

    @staticmethod
    def remove_special_content(text: str, remove_special_blocks: bool = None) -> str:
        """
        å¦‚æœ remove_special_blocks ç‚º Trueï¼Œå‰‡ç§»é™¤ç¨‹å¼ç¢¼ã€æ•¸å­¸å…¬å¼å’Œè¡¨æ ¼å€å¡Šã€‚
        """
        if not remove_special_blocks or not isinstance(text, str):
            return text

        # ç§»é™¤ç¨‹å¼ç¢¼å€å¡Š
        text = re.sub(r'```.*?```', '[CODE_BLOCK]', text, flags=re.DOTALL)
        # ç§»é™¤æ•¸å­¸å…¬å¼å€å¡Š
        text = re.sub(r'\$\$.*?\$\$', '[MATH_BLOCK]', text, flags=re.DOTALL)
        # ç§»é™¤ Markdown è¡¨æ ¼
        # text = re.sub(r"^\s*\|.*\|.*[\r\n]+^\s*\|[-|: \t]+\|.*[\r\n]+(?:^\s*\|.*\|.*\s*[\r\n]?)+", '[TABLE_BLOCK]', text, flags=re.MULTILINE)
        text = re.sub(r'((?:\|.*\|[\r\n\s]*){2,})', '[TABLE_BLOCK]', text)
        return text

    @staticmethod
    def calculate_jaccard_similarity(text1_processed: str, text2_processed: str) -> float:
        """
        è¨ˆç®— Jaccard Index - è¡¡é‡å…©å€‹æ–‡æœ¬çš„å­—å…ƒé‡ç–Šç¨‹åº¦.
        å‡è¨­å‚³å…¥çš„æ–‡æœ¬å·²ç¶“æ ¹æ“š REMOVE_SPECIAL_BLOCKS çš„è¨­å®šè¢«é©ç•¶è™•ç†éäº†ã€‚
        
        Args:
            text1_processed (str): å¯èƒ½å·²è™•ç†çš„ç¬¬ä¸€å€‹æ–‡æœ¬
            text2_processed (str): å¯èƒ½å·²è™•ç†çš„ç¬¬äºŒå€‹æ–‡æœ¬
            
        Returns:
            float: Jaccard ç›¸ä¼¼åº¦ (0-1ä¹‹é–“)
        """        
        if not isinstance(text1_processed, str) or not isinstance(text2_processed, str):
            return 0.0
        
        words1 = set(text1_processed.lower().split())
        words2 = set(text2_processed.lower().split())
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        if union == 0:
            return 0.0
        
        return intersection / union
    
    @staticmethod
    def count_code_blocks(text: str) -> int:
        """
        è¨ˆç®— markdown æ ¼å¼çš„ç¨‹å¼ç¢¼å€å¡Šæ•¸é‡
        
        Args:
            text (str): è¼¸å…¥æ–‡æœ¬
            
        Returns:
            int: ç¨‹å¼ç¢¼å€å¡Šæ•¸é‡
        """
        if not isinstance(text, str):
            return 0
        
        return str(text).count('```') // 2

    @staticmethod
    def count_math_blocks(text: str) -> int:
        """
        è¨ˆç®— markdown æ ¼å¼çš„æ•¸å­¸å…¬å¼å€å¡Šæ•¸é‡ ($$...$$)
        
        Args:
            text (str): è¼¸å…¥æ–‡æœ¬
            
        Returns:
            int: æ•¸å­¸å…¬å¼å€å¡Šæ•¸é‡
        """
        if not isinstance(text, str):
            return 0
        math_pattern = re.compile(r'\\$\\$.*?\\$\\$', re.DOTALL)
        return len(math_pattern.findall(text))

    @staticmethod
    def count_table_blocks(text: str) -> int:
        """
        è¨ˆç®— markdown æ ¼å¼çš„è¡¨æ ¼æ•¸é‡
        
        Args:
            text (str): è¼¸å…¥æ–‡æœ¬
            
        Returns:
            int: è¡¨æ ¼æ•¸é‡
        """
        if not isinstance(text, str):
            return 0
        table_pattern = re.compile(r'((?:\\|.*?\\|[\\r\\n\\s]*){2,})')
        return len(table_pattern.findall(text))
    
    @staticmethod
    def calculate_content_blocks_diff(text1_raw: str, text2_raw: str) -> int:
        """
        è¨ˆç®— markdown æ ¼å¼çš„ç¨‹å¼ç¢¼ã€æ•¸å­¸å…¬å¼å’Œè¡¨æ ¼å€å¡Šçš„ç¸½æ•¸é‡å·®å€¼ã€‚
        å§‹çµ‚åœ¨åŸå§‹æ–‡æœ¬ä¸Šæ“ä½œã€‚
        Args:
            text1_raw (str): åŸå§‹ç¬¬ä¸€å€‹æ–‡æœ¬ (response_a)
            text2_raw (str): åŸå§‹ç¬¬äºŒå€‹æ–‡æœ¬ (response_b)
            
        Returns:
            int: å…§å®¹å€å¡Šç¸½æ•¸é‡å·®å€¼ (response_a - response_b)
        """
        count1_code = MetadataFeatures.count_code_blocks(text1_raw)
        count2_code = MetadataFeatures.count_code_blocks(text2_raw)
        code_diff = count1_code - count2_code

        count1_math = MetadataFeatures.count_math_blocks(text1_raw)
        count2_math = MetadataFeatures.count_math_blocks(text2_raw)
        math_diff = count1_math - count2_math

        count1_table = MetadataFeatures.count_table_blocks(text1_raw)
        count2_table = MetadataFeatures.count_table_blocks(text2_raw)
        table_diff = count1_table - count2_table
        
        return code_diff + math_diff + table_diff
    
    @staticmethod
    def calculate_length_diff(text1_raw: str, text2_raw: str) -> int:
        """
        è¨ˆç®—å›æ‡‰é•·åº¦å·®å€¼ã€‚å§‹çµ‚åœ¨åŸå§‹æ–‡æœ¬ä¸Šæ“ä½œã€‚
        
        Args:
            text1_raw (str): åŸå§‹ç¬¬ä¸€å€‹æ–‡æœ¬ (response_a)
            text2_raw (str): åŸå§‹ç¬¬äºŒå€‹æ–‡æœ¬ (response_b)
            
        Returns:
            int: é•·åº¦å·®å€¼ (response_a - response_b)
        """
        if not isinstance(text1_raw, str) or not isinstance(text2_raw, str):
            return 0
        
        return len(text1_raw) - len(text2_raw)
    
    @staticmethod
    def calculate_ttr(text_processed: str) -> float:
        """
        è¨ˆç®— Type-Token Ratio (TTR) - è©å½™è±å¯Œåº¦æŒ‡æ¨™ã€‚
        å‡è¨­å‚³å…¥çš„æ–‡æœ¬å·²ç¶“æ ¹æ“š REMOVE_SPECIAL_BLOCKS çš„è¨­å®šè¢«é©ç•¶è™•ç†éäº†ã€‚
        
        Args:
            text_processed (str): å¯èƒ½å·²è™•ç†çš„è¼¸å…¥æ–‡æœ¬
            
        Returns:
            float: TTR å€¼ (0-1ä¹‹é–“)
        """
        if not isinstance(text_processed, str) or len(text_processed.strip()) == 0:
            return 0.0
        
        words = text_processed.lower().split()
        
        if len(words) == 0:
            return 0.0
        
        unique_words = len(set(words))
        total_words = len(words)
        
        return unique_words / total_words
    
    @staticmethod
    def calculate_ttr_diff(text1_processed: str, text2_processed: str) -> float:
        """
        è¨ˆç®— TTR å·®å€¼ã€‚
        å‡è¨­å‚³å…¥çš„æ–‡æœ¬å·²ç¶“æ ¹æ“š REMOVE_SPECIAL_BLOCKS çš„è¨­å®šè¢«é©ç•¶è™•ç†éäº†ã€‚
        
        Args:
            text1_processed (str): å¯èƒ½å·²è™•ç†çš„ç¬¬ä¸€å€‹æ–‡æœ¬ (response_a)
            text2_processed (str): å¯èƒ½å·²è™•ç†çš„ç¬¬äºŒå€‹æ–‡æœ¬ (response_b)
            
        Returns:
            float: TTR å·®å€¼ (response_a - response_b)
        """        
        ttr1 = MetadataFeatures.calculate_ttr(text1_processed)
        ttr2 = MetadataFeatures.calculate_ttr(text2_processed)
        return ttr1 - ttr2
    
    @staticmethod
    def extract_core_features(row: pd.Series) -> Dict[str, float]:
        """
        æå–äº”å€‹æ ¸å¿ƒå…ƒæ•¸æ“šç‰¹å¾µï¼ˆæ›´æ–°ç‚º5å€‹ç‰¹å¾µä»¥åŒ¹é…æ¨¡å‹æœŸæœ›ï¼‰
        
        Args:
            row (pd.Series): åŒ…å« prompt, response_a, response_b çš„æ•¸æ“šè¡Œ
            
        Returns:
            Dict[str, float]: åŒ…å«äº”å€‹æ ¸å¿ƒç‰¹å¾µçš„å­—å…¸
        """
        response_a_raw = str(row.get('response_a', ''))
        response_b_raw = str(row.get('response_b', ''))

        # æ ¹æ“šé–‹é—œæ±ºå®šæ˜¯å¦é è™•ç†æ–‡æœ¬ (åƒ…ç”¨æ–¼ Jaccard å’Œ TTR)
        response_a_for_jaccard_ttr = MetadataFeatures.remove_special_content(response_a_raw)
        response_b_for_jaccard_ttr = MetadataFeatures.remove_special_content(response_b_raw)

        # è¨ˆç®—åŸå§‹å·®ç•°å€¼
        length_diff = MetadataFeatures.calculate_length_diff(response_a_raw, response_b_raw)
        content_blocks_diff = MetadataFeatures.calculate_content_blocks_diff(response_a_raw, response_b_raw)

        # ä½¿ç”¨å°æ•¸è®Šæ›ä¾†ç¸®æ”¾ç‰¹å¾µï¼ŒåŒæ™‚ä¿ç•™æ­£è² è™Ÿ
        scaled_length_diff = np.sign(length_diff) * np.log1p(abs(length_diff))
        scaled_content_blocks_diff = np.sign(content_blocks_diff) * np.log1p(abs(content_blocks_diff))

        # è¨ˆç®—TTRå€¼
        ttr_a = MetadataFeatures.calculate_ttr(response_a_for_jaccard_ttr)
        ttr_b = MetadataFeatures.calculate_ttr(response_b_for_jaccard_ttr)
        
        # è¨ˆç®—TTRæ¯”å€¼ï¼ˆæ–°å¢çš„ç¬¬5å€‹ç‰¹å¾µï¼‰
        ttr_ratio = max(ttr_a, ttr_b) / max(min(ttr_a, ttr_b), 0.001) if min(ttr_a, ttr_b) > 0 else 1.0

        core_features = {
            # Jaccard å’Œ TTR ä½¿ç”¨å¯èƒ½è¢«è™•ç†éçš„æ–‡æœ¬
            'jaccard_index': MetadataFeatures.calculate_jaccard_similarity(
                                response_a_for_jaccard_ttr, 
                                response_b_for_jaccard_ttr
                             ),
            'ttr_diff': MetadataFeatures.calculate_ttr_diff(
                                response_a_for_jaccard_ttr, 
                                response_b_for_jaccard_ttr
                        ),
            
            # Content blocks diff å’Œ Length diff ä½¿ç”¨ç¸®æ”¾å¾Œçš„å€¼
            'content_blocks_diff': scaled_content_blocks_diff,
            'length_diff': scaled_length_diff,
            
            # æ–°å¢çš„ç¬¬5å€‹ç‰¹å¾µï¼šTTRæ¯”å€¼
            'ttr_ratio': ttr_ratio
        }
        
        return core_features
    
    @staticmethod
    def extract_all_features(row: pd.Series) -> Dict[str, float]:
        """
        æå–æ‰€æœ‰å…ƒæ•¸æ“šç‰¹å¾µ (ç›®å‰èˆ‡æ ¸å¿ƒç‰¹å¾µç›¸åŒ)
        
        Args:
            row (pd.Series): åŒ…å« prompt, response_a, response_b çš„æ•¸æ“šè¡Œ
            
        Returns:
            Dict[str, float]: åŒ…å«æ‰€æœ‰å…ƒæ•¸æ“šç‰¹å¾µçš„å­—å…¸
        """
        # ç›®å‰æ‰€æœ‰ç‰¹å¾µå°±æ˜¯æ ¸å¿ƒç‰¹å¾µ, å‘¼å«æ›´æ–°å¾Œçš„ extract_core_features
        return MetadataFeatures.extract_core_features(row)

    @staticmethod
    def _generate_cache_key(df: pd.DataFrame, feature_type: str) -> str:
        """
        ç‚º DataFrame å’Œç‰¹å¾µé¡å‹ç”Ÿæˆä¸€å€‹å”¯ä¸€çš„å¿«å–éµã€‚
        """
        # ä½¿ç”¨è¡Œæ•¸å’Œå‰å¹¾è¡Œçš„éƒ¨åˆ†å…§å®¹ä¾†ç”Ÿæˆä¸€å€‹ç›¸å°ç©©å®šçš„é›œæ¹Šå€¼
        # æ³¨æ„ï¼šé€™ä¸æ˜¯ä¸€å€‹å®Œç¾çš„é›œæ¹Šï¼Œå¦‚æœ DataFrame å…§å®¹æœ‰ç´°å¾®è®ŠåŒ–ä½†è¡Œæ•¸å’Œé–‹é ­ä¸è®Šï¼Œå¯èƒ½æœƒå°è‡´å¿«å–èª¤åˆ¤
        # æ›´å¥å£¯çš„æ–¹æ³•å¯èƒ½éœ€è¦é›œæ¹Šæ•´å€‹ DataFrame çš„å…§å®¹ï¼Œä½†æœƒæ›´è€—æ™‚
        hasher = hashlib.md5()
        hasher.update(str(len(df)).encode())
        if not df.empty:
            # å–æ¨£å‰5è¡Œï¼Œæ¯è¡Œå–å‰100å€‹å­—å…ƒä¾†è¨ˆç®—é›œæ¹Š
            sample_data = "".join(df.head(5).to_string(index=False, header=False, max_colwidth=100).split())
            hasher.update(sample_data.encode())
        hasher.update(feature_type.encode())
        return hasher.hexdigest()

    @staticmethod
    def add_metadata_features_to_dataframe(df: pd.DataFrame, feature_type: str = 'core') -> pd.DataFrame:
        """
        å°‡å…ƒæ•¸æ“šç‰¹å¾µæ·»åŠ åˆ°æ•¸æ“šæ¡†ä¸­ï¼Œä¸¦ä½¿ç”¨å¿«å–æ©Ÿåˆ¶ã€‚
        
        Args:
            df (pd.DataFrame): åŸå§‹æ•¸æ“šæ¡†
            feature_type (str): ç‰¹å¾µé¡å‹ (\'core\' æˆ– \'all\')
            
        Returns:
            pd.DataFrame: æ·»åŠ äº†å…ƒæ•¸æ“šç‰¹å¾µçš„æ•¸æ“šæ¡†
        """
        print(f"  - é–‹å§‹æå– {feature_type} å…ƒæ•¸æ“šç‰¹å¾µ...")

        # ç¢ºä¿å¿«å–ç›®éŒ„å­˜åœ¨
        if not os.path.exists(CACHE_DIR):
            os.makedirs(CACHE_DIR)
            print(f"  - å‰µå»ºå¿«å–ç›®éŒ„: {CACHE_DIR}")

        # ç”Ÿæˆå¿«å–æª”æ¡ˆè·¯å¾‘
        cache_key = MetadataFeatures._generate_cache_key(df, feature_type)
        cache_file_path = os.path.join(CACHE_DIR, f"{cache_key}_{feature_type}.pkl")
        
        # æª¢æŸ¥å¿«å–æ˜¯å¦å­˜åœ¨
        if os.path.exists(cache_file_path):
            try:
                print(f"  - ç™¼ç¾å¿«å–æª”æ¡ˆ: {cache_file_path}ï¼Œæ­£åœ¨è¼‰å…¥...")
                # è¼‰å…¥æ™‚ï¼Œåªè¼‰å…¥ç‰¹å¾µåˆ—ï¼Œé¿å…é‡è¤‡è¼‰å…¥åŸå§‹ df çš„æ¬„ä½
                cached_features_df = pd.read_pickle(cache_file_path)
                
                # é€²è¡Œæ›´å¯é çš„é©—è­‰ï¼šæª¢æŸ¥å¿«å–ä¸­çš„æ¬„ä½æ˜¯å¦éƒ½æ˜¯é æœŸçš„ç‰¹å¾µæ¬„ä½
                # å‡è¨­æ ¸å¿ƒç‰¹å¾µæ˜¯å›ºå®šçš„
                expected_core_features = ['jaccard_index','ttr_diff', 'ttr_ratio', 'content_blocks_diff', 'length_diff']
                # 'all' features ç›®å‰èˆ‡ core ç›¸åŒï¼Œå¦‚æœå°‡ä¾†ä¸åŒï¼Œé€™è£¡éœ€è¦èª¿æ•´
                expected_features_set = set(expected_core_features)

                if isinstance(cached_features_df, pd.DataFrame) and not cached_features_df.empty and set(cached_features_df.columns) == expected_features_set and len(cached_features_df) == len(df):
                    # å°‡å¿«å–çš„ç‰¹å¾µ DataFrame èˆ‡åŸå§‹ DataFrame (ä¸åŒ…å«å·²å­˜åœ¨çš„ç‰¹å¾µåˆ—ï¼Œä»¥é˜²è¬ä¸€) åˆä½µ
                    # å…ˆç§»é™¤åŸå§‹ df ä¸­å¯èƒ½å·²å­˜åœ¨çš„åŒåç‰¹å¾µåˆ—ï¼Œå†åˆä½µ
                    df_copy = df.copy()
                    for col in expected_features_set:
                        if col in df_copy.columns:
                            df_copy = df_copy.drop(columns=[col])
                    
                    df_enhanced = pd.concat([df_copy.reset_index(drop=True), cached_features_df.reset_index(drop=True)], axis=1)
                    print(f"  - æˆåŠŸå¾å¿«å–è¼‰å…¥ {len(cached_features_df.columns)} å€‹å…ƒæ•¸æ“šç‰¹å¾µ")
                    print(f"  - å·²è¼‰å…¥ç‰¹å¾µ: {list(cached_features_df.columns)}")
                    return df_enhanced
                else:
                    print(f"  - å¿«å–æª”æ¡ˆç„¡æ•ˆ (æ¬„ä½ä¸ç¬¦ã€ç‚ºç©ºæˆ–é•·åº¦ä¸åŒ¹é…)ï¼Œå°‡é‡æ–°è¨ˆç®—ç‰¹å¾µã€‚")
                    if not isinstance(cached_features_df, pd.DataFrame) or cached_features_df.empty:
                         print(f"    - åŸå› : å¿«å–æª”æ¡ˆä¸æ˜¯æœ‰æ•ˆçš„ DataFrame æˆ–ç‚ºç©ºã€‚")
                    elif set(cached_features_df.columns) != expected_features_set:
                         print(f"    - åŸå› : å¿«å–æ¬„ä½èˆ‡é æœŸä¸ç¬¦ã€‚é æœŸ: {expected_features_set}, å¯¦éš›: {set(cached_features_df.columns)}")
                    elif len(cached_features_df) != len(df):
                         print(f"    - åŸå› : å¿«å–é•·åº¦èˆ‡ç•¶å‰ DataFrame é•·åº¦ä¸ç¬¦ã€‚é æœŸ: {len(df)}, å¯¦éš›: {len(cached_features_df)}")

            except Exception as e:
                print(f"  - è¼‰å…¥å¿«å–æª”æ¡ˆå¤±æ•—: {e}ï¼Œå°‡é‡æ–°è¨ˆç®—ç‰¹å¾µã€‚")

        df_enhanced = df.copy()
        
        if feature_type == 'core':
            features_list = df_enhanced.progress_apply(MetadataFeatures.extract_core_features, axis=1).tolist()
        else: # 'all'
            features_list = df_enhanced.progress_apply(MetadataFeatures.extract_all_features, axis=1).tolist()
        
        features_df = pd.DataFrame(features_list) # é€™åªåŒ…å«æ–°æå–çš„ç‰¹å¾µåˆ—
        
        # å„²å­˜åˆ°å¿«å– (åªå„²å­˜ç‰¹å¾µ DataFrame)
        try:
            features_df.to_pickle(cache_file_path)
            print(f"  - ç‰¹å¾µå·²è¨ˆç®—ä¸¦å„²å­˜åˆ°å¿«å–: {cache_file_path}")
        except Exception as e:
            print(f"  - å„²å­˜ç‰¹å¾µåˆ°å¿«å–å¤±æ•—: {e}")
            
        # åˆä½µåŸå§‹ DataFrame å’Œæ–°æå–çš„ç‰¹å¾µ DataFrame
        # å…ˆç§»é™¤åŸå§‹ df ä¸­å¯èƒ½å·²å­˜åœ¨çš„åŒåç‰¹å¾µåˆ—ï¼Œå†åˆä½µ
        for col in features_df.columns:
            if col in df_enhanced.columns:
                df_enhanced = df_enhanced.drop(columns=[col])
        df_enhanced = pd.concat([df_enhanced.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)
        
        print(f"  - æˆåŠŸæå– {len(features_df.columns)} å€‹å…ƒæ•¸æ“šç‰¹å¾µ")
        print(f"  - å·²æå–ç‰¹å¾µ: {list(features_df.columns)}")
        
        return df_enhanced

    @staticmethod
    def analyze_feature_distributions(df: pd.DataFrame, feature_names: List[str]) -> Dict[str, Dict]:
        """
        åˆ†æå…ƒæ•¸æ“šç‰¹å¾µçš„åˆ†å¸ƒæƒ…æ³
        
        Args:
            df (pd.DataFrame): åŒ…å«ç‰¹å¾µçš„æ•¸æ“šæ¡†
            feature_names (List[str]): è¦åˆ†æçš„ç‰¹å¾µåç¨±åˆ—è¡¨
            
        Returns:
            Dict[str, Dict]: æ¯å€‹ç‰¹å¾µçš„çµ±è¨ˆä¿¡æ¯
        """
        print("  - é–‹å§‹åˆ†æå…ƒæ•¸æ“šç‰¹å¾µåˆ†å¸ƒ...")
        
        stats = {}
        
        for feature in feature_names:
            if feature in df.columns:
                feature_data = df[feature].dropna()
                stats[feature] = {
                    'count': len(feature_data),
                    'mean': feature_data.mean(),
                    'median': feature_data.median(),
                    'std': feature_data.std(),
                    'min': feature_data.min(),
                    'max': feature_data.max(),
                    'percentile_25': feature_data.quantile(0.25),
                    'percentile_75': feature_data.quantile(0.75),
                    'null_count': df[feature].isnull().sum()
                }
                
                print(f"    {feature}: å‡å€¼={stats[feature]['mean']:.3f}, "
                      f"æ¨™æº–å·®={stats[feature]['std']:.3f}, "
                      f"ç¯„åœ=[{stats[feature]['min']:.3f}, {stats[feature]['max']:.3f}]")
        
        return stats

    @staticmethod
    def create_feature_vector(features_dict: Dict[str, float], feature_order: List[str] = None) -> List[float]:
        """
        å°‡ç‰¹å¾µå­—å…¸è½‰æ›ç‚ºæœ‰åºçš„ç‰¹å¾µå‘é‡
        
        Args:
            features_dict (Dict[str, float]): ç‰¹å¾µå­—å…¸
            feature_order (List[str]): ç‰¹å¾µé †åºï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é»˜èªé †åº
            
        Returns:
            List[float]: ç‰¹å¾µå‘é‡
        """
        if feature_order is None:
            feature_order = ['jaccard_index', 'ttr_diff', 'ttr_ratio', 'content_blocks_diff', 'length_diff'] # Adjusted order for clarity
        
        feature_vector = []
        for feature_name in feature_order:
            value = features_dict.get(feature_name, 0.0)
            if np.isinf(value):
                value = 10.0  
            elif np.isnan(value):
                value = 0.0
            feature_vector.append(float(value))
        
        return feature_vector

    @staticmethod
    def _calculate_features(text: str) -> dict:
        """
        è¨ˆç®—æ–‡æœ¬çš„æ‰€æœ‰å…ƒæ•¸æ“šç‰¹å¾µ
        
        Args:
            text (str): è¼¸å…¥æ–‡æœ¬
            
        Returns:
            dict: åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„å­—å…¸
        """
        if not isinstance(text, str):
            return {}
        
        # è¨ˆç®—å­—å…ƒæ•¸ã€è©å½™æ•¸ã€å¥å­æ•¸
        char_count = len(text)
        word_count = len(text.split())
        sentence_count = text.count('.') + text.count('!') + text.count('?')
        
        # è¨ˆç®—å¹³å‡è©å½™é•·åº¦
        avg_word_length = char_count / word_count if word_count > 0 else 0
        
        # è¨ˆç®—ç¨ç‰¹è©å½™æ¯”ä¾‹
        unique_word_ratio = MetadataFeatures.calculate_ttr(text)
        
        # åœç”¨è©å’Œæ¨™é»ç¬¦è™Ÿè¨ˆæ•¸
        stopwords = set(['çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä»–', 'å¥¹', 'å®ƒ', 'é€™', 'é‚£', 'å€‹', 'äº†', 'ä¸', 'äºº', 'éƒ½', 'èªª', 'è¦', 'å»', 'å—'])
        words = text.split()
        stopword_count = len([word for word in words if word in stopwords])
        punctuation_count = len(re.findall(r'[^\w\s]', text))
        
        # ç‰¹æ®Šå­—å…ƒè¨ˆæ•¸ (ä¾‹å¦‚ï¼š@, #, $, %, ^, &, *)
        special_char_count = len(re.findall(r'[@#$%^&*]', text))
        
        # URL è¨ˆæ•¸
        url_count = len(re.findall(r'http[s]?://\S+', text))
        
        # ç¨‹å¼ç¢¼å€å¡Šã€æ•¸å­¸å…¬å¼å’Œè¡¨æ ¼å€å¡Šè¨ˆæ•¸
        code_block_count = MetadataFeatures.count_code_blocks(text) + MetadataFeatures.count_math_blocks(text) + MetadataFeatures.count_table_blocks(text)
        
        # è¿”å›æ‰€æœ‰ç‰¹å¾µçš„å­—å…¸
        return {
            'char_count': char_count,
            'word_count': word_count,
            'sentence_count': sentence_count,
            'avg_word_length': avg_word_length,
            'unique_word_ratio': unique_word_ratio,
            'stopword_count': stopword_count,
            'punctuation_count': punctuation_count,
            'special_char_count': special_char_count,
            'code_block_count': code_block_count,
            'url_count': url_count
        }

    @staticmethod
    def get_feature_columns(feature_type: str = 'core') -> list:
        """Returns the list of metadata feature column names."""
        # å®šç¾©æ ¸å¿ƒç‰¹å¾µåˆ—è¡¨ï¼Œé€™èˆ‡ extract_core_features ä¸­å¯¦éš›è¨ˆç®—çš„ç‰¹å¾µé †åºä¸€è‡´
        # é †åºå¿…é ˆèˆ‡ extract_core_features è¿”å›çš„å­—å…¸éµé †åºå®Œå…¨åŒ¹é…
        core_features = ['jaccard_index', 'ttr_diff', 'ttr_ratio', 'content_blocks_diff', 'length_diff']

        # æ ¹æ“š `extract_all_features` ç•¶å‰çš„å¯¦ç¾ï¼Œ'all' å’Œ 'core' è¿”å›ç›¸åŒçš„ç‰¹å¾µã€‚
        # å› æ­¤ï¼Œæˆ‘å€‘çµ±ä¸€è¿”å› core_features åˆ—è¡¨ä»¥ç¢ºä¿ä¸€è‡´æ€§ã€‚
        if feature_type == 'core':
            return core_features
        elif feature_type == 'all':
            # `extract_all_features` ç›®å‰ç›´æ¥èª¿ç”¨ `extract_core_features`ï¼Œ
            # æ‰€ä»¥ 'all' é¡å‹ä¹Ÿæ‡‰è©²è¿”å›ç›¸åŒçš„ç‰¹å¾µåˆ—è¡¨ã€‚
            return core_features
        else:
            return []

    @staticmethod
    def add_metadata_features(row, feature_type: str = 'core') -> pd.Series:
        """Calculates metadata features for a single row (prompt, response_a, response_b)."""
        response_a = str(row.get('response_a', ''))
        response_b = str(row.get('response_b', ''))
        
        # è¨ˆç®—TTRæ¯”å€¼
        ttr_a = MetadataFeatures.calculate_ttr(MetadataFeatures.remove_special_content(response_a))
        ttr_b = MetadataFeatures.calculate_ttr(MetadataFeatures.remove_special_content(response_b))
        ttr_ratio = max(ttr_a, ttr_b) / max(min(ttr_a, ttr_b), 0.001) if min(ttr_a, ttr_b) > 0 else 1.0
        
        return pd.Series({
            'jaccard_index': MetadataFeatures.calculate_jaccard_similarity(response_a, response_b),
            'ttr_diff': MetadataFeatures.calculate_ttr_diff(response_a, response_b),
            'content_blocks_diff': MetadataFeatures.calculate_content_blocks_diff(response_a, response_b),
            'length_diff': MetadataFeatures.calculate_length_diff(response_a, response_b),
            'ttr_ratio': ttr_ratio
        })


# <<<<<<< ADD <<<<<<<

# --------------------------------------------------------------------------
# Embedded DualTowerPairClassifier (èˆ‡ fine_tuning.py å®Œå…¨ä¸€è‡´)
# --------------------------------------------------------------------------
class DualTowerPairClassifier(nn.Module):
    """
    Dual-Encoder / Two-Tower æ¨¡å‹ + Metadata ç‰¹å¾µè™•ç†
    - å…±ç”¨ä¸€åº§ Transformer Encoderï¼Œå„è‡ªç·¨ç¢¼ Promptã€Response Aã€Response B  
    - ç‰¹å¾µå‘é‡: [v_p, v_a, |v_p âˆ’ v_a|, v_b, |v_p âˆ’ v_b|, metadata_features]
    - metadata é€é meta_path å¾4ç¶­å‡åˆ°768ç¶­
    - æœ€çµ‚ç‰¹å¾µ: 768 Ã— 6 = 4608 ç¶­
    - 3-é¡ softmax â†’ 0=A wins, 1=B wins, 2=Tie
    """

    def __init__(
        self,
        base_model: str = "distilbert-base-uncased",
        hidden_size: int = 768,
        dropout: float = 0.2,
        metadata_feature_size: int = 5,  # metadata åŸå§‹ç¶­åº¦ï¼ˆä¿®æ­£ç‚º5ï¼‰
        metadata_fusion: str = 'dual_path',  # metadata èåˆæ–¹å¼
        config_dict: dict = None,  # ç›´æ¥å‚³å…¥é…ç½®å­—å…¸ç”¨æ–¼é›¢ç·šè¼‰å…¥
    ):
        super().__init__()
        
        # å¦‚æœæä¾›äº†é…ç½®å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨å®ƒå‰µå»º encoder
        if config_dict is not None:
            from transformers import DistilBertConfig, DistilBertModel
            encoder_config = DistilBertConfig(**config_dict)
            self.encoder = DistilBertModel(encoder_config)
            print(f"    ğŸ’¡ ä½¿ç”¨æä¾›çš„é…ç½®å‰µå»º DistilBert encoder")
        else:
            # å¦å‰‡ä½¿ç”¨æ¨™æº–æ–¹å¼ï¼ˆå¯èƒ½éœ€è¦ç¶²çµ¡ï¼‰
            try:
                self.encoder = AutoModel.from_pretrained(base_model, local_files_only=True)
                print(f"    ğŸ’¡ ä½¿ç”¨æœ¬åœ°ç·©å­˜è¼‰å…¥ {base_model}")
            except Exception as e:
                print(f"    ğŸ’¡ æœ¬åœ°è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨é è¨­é…ç½®: {e}")
                # å¦‚æœæœ¬åœ°è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­çš„ DistilBert é…ç½®
                from transformers import DistilBertConfig, DistilBertModel
                encoder_config = DistilBertConfig(
                    vocab_size=30522,
                    dim=hidden_size,
                    n_layers=6,
                    n_heads=12,
                    hidden_dim=3072,
                    dropout=0.1,
                    attention_dropout=0.1,
                    max_position_embeddings=512,
                    initializer_range=0.02
                )
                self.encoder = DistilBertModel(encoder_config)
                print(f"    ğŸ’¡ ä½¿ç”¨é è¨­ DistilBert é…ç½®å‰µå»º encoder")
        
        self.dropout = nn.Dropout(dropout)
        
        # Metadata è™•ç†è·¯å¾‘ - èˆ‡ fine_tuning.py å®Œå…¨ä¸€è‡´
        self.meta_path = None
        if metadata_fusion == 'dual_path' and metadata_feature_size > 0:
            self.meta_path = nn.Sequential(
                nn.Linear(metadata_feature_size, hidden_size),
                nn.ReLU(),
                nn.Linear(hidden_size, hidden_size),
                nn.ReLU()
            )
            print(f"    ğŸ’¡ å‰µå»º metadata è™•ç†è·¯å¾‘: {metadata_feature_size} -> {hidden_size} -> {hidden_size}")
        
        # åˆ†é¡å™¨è¼¸å…¥ç¶­åº¦: åŸºç¤ç‰¹å¾µ(768Ã—5) + metadataç‰¹å¾µ(768) = 768Ã—6 = 4608
        classifier_input_dim = hidden_size * 6 if self.meta_path else hidden_size * 5
        
        self.classifier = nn.Sequential(
            nn.Linear(classifier_input_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 3),
        )
        
        print(f"    ğŸ’¡ åˆ†é¡å™¨è¼¸å…¥ç¶­åº¦: {classifier_input_dim} (æœŸæœ› 4608)")

    def encode(self, ids: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """å›å‚³ [CLS] embedding (batch, hidden)"""
        out = self.encoder(
            input_ids=ids, attention_mask=mask, return_dict=True
        )
        return out.last_hidden_state[:, 0]

    def forward(
        self,
        input_ids: torch.Tensor = None,
        attention_mask: torch.Tensor = None,
        p_input_ids: torch.Tensor = None,
        p_attention_mask: torch.Tensor = None,
        a_input_ids: torch.Tensor = None,
        a_attention_mask: torch.Tensor = None,
        b_input_ids: torch.Tensor = None,
        b_attention_mask: torch.Tensor = None,
        metadata_features: torch.Tensor = None,
        labels: Optional[torch.Tensor] = None,
    ):
        """
        æ”¯æŒå…©ç¨®è¼¸å…¥æ ¼å¼ï¼š
        1. è¯åˆè¼¸å…¥ï¼šinput_ids + attention_mask + metadata_features (æ¨ç†æ™‚ä½¿ç”¨)
        2. åˆ†é›¢è¼¸å…¥ï¼šp_input_ids, a_input_ids, b_input_ids + metadata_features (è¨“ç·´æ™‚ä½¿ç”¨)
        """
        
        if input_ids is not None and attention_mask is not None:
            # è¯åˆè¼¸å…¥æ ¼å¼ï¼ˆæ¨ç†æ™‚ä½¿ç”¨ï¼‰
            cls_embedding = self.encode(input_ids, attention_mask)
            feat = cls_embedding
                
        else:
            # åˆ†é›¢è¼¸å…¥æ ¼å¼ï¼ˆè¨“ç·´æ™‚ä½¿ç”¨ï¼‰
            v_p = self.encode(p_input_ids, p_attention_mask)
            v_a = self.encode(a_input_ids, a_attention_mask)
            v_b = self.encode(b_input_ids, b_attention_mask)

            # æ‹¼æ¥åŸºç¤ç‰¹å¾µå‘é‡ï¼š[v_p, v_a, |v_p - v_a|, v_b, |v_p - v_b|] = 768Ã—5
            feat = torch.cat(
                [v_p, v_a, torch.abs(v_p - v_a), v_b, torch.abs(v_p - v_b)], dim=-1
            )
        
        # è™•ç† metadata ç‰¹å¾µ
        if self.meta_path and metadata_features is not None:
            # é€šé meta_path å‡ç¶­ï¼š4 -> 768 -> 768
            meta_feat = self.meta_path(metadata_features)
            # æ‹¼æ¥ï¼šåŸºç¤ç‰¹å¾µ(768Ã—5) + metadataç‰¹å¾µ(768) = 768Ã—6 = 4608
            feat = torch.cat([feat, meta_feat], dim=-1)
        elif self.meta_path:
            # å¦‚æœæ¨¡å‹æœ‰ meta_path ä½†æ²’æœ‰æä¾› metadataï¼Œç”¨é›¶å¡«å……
            batch_size = feat.shape[0]
            zero_meta = torch.zeros(batch_size, self.encoder.config.dim, device=feat.device)
            feat = torch.cat([feat, zero_meta], dim=-1)
        
        logits = self.classifier(self.dropout(feat))

        loss = None
        if labels is not None:
            loss = nn.CrossEntropyLoss()(logits, labels)

        return {"loss": loss, "logits": logits}
    
    @classmethod
    def from_pretrained(cls, model_path, local_files_only=True):
        """å¾é è¨“ç·´è·¯å¾‘è¼‰å…¥æ¨¡å‹ (å®Œå…¨é›¢ç·šï¼Œå…¼å®¹ Kaggle ç’°å¢ƒ)"""
        print(f"    ğŸ” å¾è·¯å¾‘è¼‰å…¥æ¨¡å‹: {model_path}")
        
        # æª¢æŸ¥å¿…è¦æ–‡ä»¶
        safetensors_path = os.path.join(model_path, 'model.safetensors')
        config_path = os.path.join(model_path, 'config.json')
        tokenizer_config_path = os.path.join(model_path, 'tokenizer_config.json')
        
        if not os.path.exists(safetensors_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ¬Šé‡æ–‡ä»¶: {safetensors_path}")
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {config_path}")
        
        # è¼‰å…¥æ¬Šé‡
        try:
            from safetensors.torch import load_file
            state_dict = load_file(safetensors_path)
            print(f"    âœ… ä½¿ç”¨ SafeTensors æ ¼å¼è¼‰å…¥æ¬Šé‡")
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£ safetensors: pip install safetensors")
        
        # åˆ†ææ¬Šé‡çµæ§‹ï¼Œç¢ºå®šæ¨¡å‹é…ç½®
        has_meta_path = any('meta_path' in key for key in state_dict.keys())
        classifier_input_dim = None
        if 'classifier.0.weight' in state_dict:
            classifier_input_dim = state_dict['classifier.0.weight'].shape[1]
        
        print(f"    ğŸ” åˆ†ææ¨¡å‹çµæ§‹:")
        print(f"      - åŒ…å« meta_path: {has_meta_path}")
        print(f"      - åˆ†é¡å™¨è¼¸å…¥ç¶­åº¦: {classifier_input_dim}")
        
        # è®€å–é…ç½®æ–‡ä»¶
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # æª¢æŸ¥tokenizeré…ç½®ä»¥ç¢ºå®šæ¨¡å‹é¡å‹
        model_type = 'distilbert'  # é»˜èª
        if os.path.exists(tokenizer_config_path):
            with open(tokenizer_config_path, 'r') as f:
                tokenizer_config = json.load(f)
            tokenizer_class = tokenizer_config.get('tokenizer_class', '')
            if 'Deberta' in tokenizer_class:
                model_type = 'deberta'
                print(f"    ğŸ” æª¢æ¸¬åˆ° DeBERTa æ¨¡å‹")
            elif 'DistilBert' in tokenizer_class:
                model_type = 'distilbert'
                print(f"    ğŸ” æª¢æ¸¬åˆ° DistilBERT æ¨¡å‹")
        
        # ç¢ºå®šæ¨¡å‹åƒæ•¸
        if has_meta_path and classifier_input_dim == 4608:
            hidden_size = 768
            metadata_feature_size = 5
            print(f"      - æª¢æ¸¬åˆ°å¸¶metadataçš„dual-toweræ¨¡å‹")
        elif classifier_input_dim == 3840:
            # æ²’æœ‰ metadataï¼š3840 = 768Ã—5
            hidden_size = 768
            metadata_feature_size = 0
            has_meta_path = False
            print(f"      - æª¢æ¸¬åˆ°åŸºç¤dual-toweræ¨¡å‹")
        else:
            # é€šç”¨æƒ…æ³
            hidden_size = classifier_input_dim // 6 if has_meta_path else classifier_input_dim // 5
            metadata_feature_size = 5 if has_meta_path else 0
            print(f"      - é€šç”¨é…ç½®: hidden_size={hidden_size}, meta_path={has_meta_path}")
        
        # æ ¹æ“šæ¨¡å‹é¡å‹æ§‹å»ºé…ç½®å­—å…¸
        if model_type == 'deberta':
            # DeBERTa é…ç½®
            encoder_config_dict = {
                'vocab_size': config.get('vocab_size', 128100),
                'hidden_size': hidden_size,
                'num_hidden_layers': config.get('num_hidden_layers', 12),
                'num_attention_heads': config.get('num_attention_heads', 12),
                'intermediate_size': config.get('intermediate_size', 3072),
                'hidden_act': config.get('hidden_act', 'gelu'),
                'hidden_dropout_prob': config.get('hidden_dropout_prob', 0.1),
                'attention_probs_dropout_prob': config.get('attention_probs_dropout_prob', 0.1),
                'max_position_embeddings': config.get('max_position_embeddings', 512),
                'type_vocab_size': config.get('type_vocab_size', 0),
                'initializer_range': config.get('initializer_range', 0.02),
                'relative_attention': config.get('relative_attention', True),
                'max_relative_positions': config.get('max_relative_positions', -1),
                'pad_token_id': config.get('pad_token_id', 0),
                'position_biased_input': config.get('position_biased_input', False)
            }
            
            # å‰µå»ºDeBERTaæ¨¡å‹
            try:
                from transformers import DebertaV2Config, DebertaV2Model
                encoder_config = DebertaV2Config(**encoder_config_dict)
                encoder = DebertaV2Model(encoder_config)
                print(f"    ğŸ’¡ ä½¿ç”¨ DeBERTa é…ç½®å‰µå»º encoder")
            except ImportError:
                raise ImportError("éœ€è¦å®‰è£ transformers æ”¯æŒ DeBERTa")
        else:
            # DistilBERT é…ç½®
            encoder_config_dict = {
                'vocab_size': config.get('vocab_size', 30522),
                'dim': hidden_size,  # DistilBert ä½¿ç”¨ 'dim' è€Œä¸æ˜¯ 'hidden_size'
                'n_layers': config.get('n_layers', 6),
                'n_heads': config.get('n_heads', 12),
                'hidden_dim': config.get('hidden_dim', 3072),
                'dropout': config.get('dropout', 0.1),
                'attention_dropout': config.get('attention_dropout', 0.1),
                'max_position_embeddings': config.get('max_position_embeddings', 512),
                'initializer_range': config.get('initializer_range', 0.02)
            }
            
            # å‰µå»ºDistilBERTæ¨¡å‹
            from transformers import DistilBertConfig, DistilBertModel
            encoder_config = DistilBertConfig(**encoder_config_dict)
            encoder = DistilBertModel(encoder_config)
            print(f"    ğŸ’¡ ä½¿ç”¨ DistilBERT é…ç½®å‰µå»º encoder")
        
        # å‰µå»ºæ¨¡å‹å¯¦ä¾‹
        metadata_fusion = 'dual_path' if has_meta_path else None
        
        # æ­£ç¢ºåˆå§‹åŒ–PyTorchæ¨¡å‹
        model = object.__new__(cls)  # å‰µå»ºå¯¦ä¾‹ä½†ä¸èª¿ç”¨__init__
        torch.nn.Module.__init__(model)  # æ‰‹å‹•èª¿ç”¨çˆ¶é¡åˆå§‹åŒ–
        
        # æ‰‹å‹•è¨­ç½®å±¬æ€§
        model.encoder = encoder
        model.dropout = torch.nn.Dropout(0.2)
        
        # Metadata è™•ç†è·¯å¾‘
        model.meta_path = None
        if metadata_fusion == 'dual_path' and metadata_feature_size > 0:
            model.meta_path = torch.nn.Sequential(
                torch.nn.Linear(metadata_feature_size, hidden_size),
                torch.nn.ReLU(),
                torch.nn.Linear(hidden_size, hidden_size),
                torch.nn.ReLU()
            )
            print(f"    ğŸ’¡ å‰µå»º metadata è™•ç†è·¯å¾‘: {metadata_feature_size} -> {hidden_size} -> {hidden_size}")
        
        # åˆ†é¡å™¨è¼¸å…¥ç¶­åº¦: åŸºç¤ç‰¹å¾µ(768Ã—5) + metadataç‰¹å¾µ(768) = 768Ã—6 = 4608
        classifier_input_dim = hidden_size * 6 if model.meta_path else hidden_size * 5
        
        model.classifier = torch.nn.Sequential(
            torch.nn.Linear(classifier_input_dim, hidden_size),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_size, 3),
        )
        
        print(f"    ğŸ’¡ åˆ†é¡å™¨è¼¸å…¥ç¶­åº¦: {classifier_input_dim} (æœŸæœ› 4608)")
        
        # æ™ºèƒ½æ¬Šé‡éæ¿¾ - åªè¼‰å…¥åŒ¹é…çš„æ¬Šé‡
        model_state_dict = model.state_dict()
        filtered_state_dict = {}
        
        for key, value in state_dict.items():
            if key in model_state_dict:
                if model_state_dict[key].shape == value.shape:
                    filtered_state_dict[key] = value
                else:
                    print(f"      âš ï¸  å½¢ç‹€ä¸åŒ¹é…ï¼Œè·³é: {key} ({model_state_dict[key].shape} vs {value.shape})")
            else:
                print(f"      ğŸ’¡ æ¨¡å‹ä¸­ç„¡æ­¤å±¤ï¼Œè·³é: {key}")
        
        # è¼‰å…¥æ¬Šé‡
        missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)
        
        if missing_keys:
            print(f"      âš ï¸  ç¼ºå°‘çš„æ¬Šé‡ (å°‡ä½¿ç”¨éš¨æ©Ÿåˆå§‹åŒ–): {len(missing_keys)} å€‹å±¤")
        if unexpected_keys:
            print(f"      ğŸ’¡ æœªä½¿ç”¨çš„æ¬Šé‡: {len(unexpected_keys)} å€‹å±¤")
        
        print(f"    âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
        return model

# --------------------------------------------------------------------------
# 1. Kaggle Environment Configuration
# --------------------------------------------------------------------------
class KaggleConfig:
    """Kaggle ç’°å¢ƒé…ç½®"""
    def __init__(self):
        # æª¢æ¸¬é‹è¡Œç’°å¢ƒ
        self.IS_KAGGLE = os.path.exists('/kaggle/input')
        
        if self.IS_KAGGLE:
            # Kaggle è·¯å¾‘è¨­ç½® (é›¢ç·šç’°å¢ƒ)
            self.MODEL_PATH = "/kaggle/input/global-best-model"  # é è¨“ç·´æ¨¡å‹è·¯å¾‘
            self.TEST_PATH = "/kaggle/input/llm-classification-finetuning/test.csv"  # æ¸¬è©¦æ•¸æ“šè·¯å¾‘
            self.OUTPUT_PATH = "/kaggle/working/submission.csv"  # è¼¸å‡ºè·¯å¾‘
            print(f"INFO: Kaggle é›¢ç·šç’°å¢ƒé…ç½®")
        else:
            # æœ¬åœ°ç’°å¢ƒè·¯å¾‘è¨­ç½®
            self.MODEL_PATH = "./global_best_model"  # æœ¬åœ°æ¨¡å‹è·¯å¾‘
            # å„ªå…ˆä½¿ç”¨æ¸¬è©¦æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡ä½¿ç”¨è¨“ç·´æ•¸æ“šçš„ä¸€éƒ¨åˆ†
            if os.path.exists("./test.csv"):
                self.TEST_PATH = "./test.csv"
            elif os.path.exists("./test_final_ttr.csv"):
                self.TEST_PATH = "./test_final_ttr.csv"
            else:
                self.TEST_PATH = "./train.csv"  # ä½¿ç”¨è¨“ç·´æ•¸æ“šé€²è¡Œæ¸¬è©¦
                print(f"    ğŸ’¡ æ¸¬è©¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°‡ä½¿ç”¨è¨“ç·´æ•¸æ“šçš„å‰100è¡Œé€²è¡ŒåŠŸèƒ½é©—è­‰")
            self.OUTPUT_PATH = "./submission.csv"  # æœ¬åœ°è¼¸å‡ºè·¯å¾‘
            print(f"INFO: æœ¬åœ°ç’°å¢ƒé…ç½®")
        
        # è¨­å‚™é…ç½®
        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ‰¹è™•ç†è¨­ç½®
        self.BATCH_SIZE = 32
        self.MAX_LENGTH = 512
        
        print(f"  - æ¨¡å‹è·¯å¾‘: {self.MODEL_PATH}")
        print(f"  - æ¸¬è©¦æ•¸æ“š: {self.TEST_PATH}")
        print(f"  - è¼¸å‡ºè·¯å¾‘: {self.OUTPUT_PATH}")
        print(f"  - ä½¿ç”¨è¨­å‚™: {self.DEVICE}")

# --------------------------------------------------------------------------
# 2. Enhanced Metadata Features with Dynamic Loading (èˆ‡ fine_tuning.py ä¸€è‡´)
# --------------------------------------------------------------------------
class SimpleMetadataFeatures:
    """
    å¢å¼·çš„å…ƒæ•¸æ“šç‰¹å¾µæå–ï¼Œèˆ‡ fine_tuning.py ä¸­çš„ MetadataFeatures æ ¸å¿ƒé‚è¼¯ä¸€è‡´
    æ”¯æŒå¾ä¿å­˜çš„æ¨¡å‹ä¸­è¼‰å…¥çµ±è¨ˆåƒæ•¸ï¼Œç¢ºä¿æ¨ç†æ™‚èˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´
    """
    
    # é è¨­çµ±è¨ˆåƒæ•¸ï¼ˆä½œç‚ºå‚™ä»½ï¼‰
    DEFAULT_FEATURE_STATS = {
        'jaccard_index': {'mean': 0.119287, 'std': 0.108648},
        'ttr_diff': {'mean': 0.120716, 'std': 0.114099},
        'content_blocks_diff': {'mean': -0.000184, 'std': 0.328517},
        'length_diff': {'mean': -0.004185, 'std': 5.951747},
        'ttr_ratio': {'mean': 1.257098, 'std': 1.679197},
    }
    
    # å°‡åœ¨è¼‰å…¥æ¨¡å‹æ™‚è¨­ç½®
    FEATURE_STATS = None
    
    @classmethod
    def load_metadata_stats(cls, model_path: str):
        """å¾æ¨¡å‹ç›®éŒ„è¼‰å…¥metadataçµ±è¨ˆåƒæ•¸"""
        stats_path = os.path.join(model_path, 'metadata_stats.json')
        
        if os.path.exists(stats_path):
            try:
                with open(stats_path, 'r') as f:
                    stats = json.load(f)
                
                # å°‡è¨“ç·´æ™‚çš„4ç‰¹å¾µæ˜ å°„åˆ°æ¨ç†æ™‚çš„5ç‰¹å¾µæ ¼å¼
                cls.FEATURE_STATS = {}
                if 'jaccard_index' in stats:
                    cls.FEATURE_STATS['jaccard_index'] = {
                        'mean': stats['jaccard_index']['mean'],
                        'std': stats['jaccard_index']['std']
                    }
                
                if 'ttr_diff' in stats:
                    # è¨“ç·´æ™‚å’Œæ¨ç†æ™‚éƒ½è¨ˆç®—åŸå§‹å·®å€¼ response_a - response_bï¼Œç„¡éœ€ä¿®æ”¹
                    cls.FEATURE_STATS['ttr_diff'] = {
                        'mean': stats['ttr_diff']['mean'],  # ç›´æ¥ä½¿ç”¨åŸå§‹å‡å€¼
                        'std': stats['ttr_diff']['std']
                    }
                
                if 'content_blocks_diff' in stats:
                    cls.FEATURE_STATS['content_blocks_diff'] = {
                        'mean': stats['content_blocks_diff']['mean'],
                        'std': stats['content_blocks_diff']['std']
                    }
                
                if 'length_diff' in stats:
                    cls.FEATURE_STATS['length_diff'] = {
                        'mean': stats['length_diff']['mean'],
                        'std': stats['length_diff']['std']
                    }
                
                # æ·»åŠ  ttr_ratioï¼ˆå¦‚æœè¨“ç·´æ™‚æ²’æœ‰ï¼Œä½¿ç”¨é è¨­å€¼ï¼‰
                if 'ttr_ratio' in stats:
                    cls.FEATURE_STATS['ttr_ratio'] = {
                        'mean': stats['ttr_ratio']['mean'],
                        'std': stats['ttr_ratio']['std']
                    }
                else:
                    cls.FEATURE_STATS['ttr_ratio'] = cls.DEFAULT_FEATURE_STATS['ttr_ratio']
                
                print(f"    âœ… è¼‰å…¥metadataçµ±è¨ˆåƒæ•¸: {stats_path}")
                print(f"    ğŸ“Š è¼‰å…¥çš„ç‰¹å¾µ: {list(cls.FEATURE_STATS.keys())}")
                return True
                
            except Exception as e:
                print(f"    âš ï¸  è¼‰å…¥metadataçµ±è¨ˆåƒæ•¸å¤±æ•—: {e}")
        
        # å¦‚æœè¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­åƒæ•¸
        print(f"    ğŸ’¡ ä½¿ç”¨é è¨­metadataçµ±è¨ˆåƒæ•¸")
        cls.FEATURE_STATS = cls.DEFAULT_FEATURE_STATS.copy()
        return False
    
    @staticmethod
    def remove_special_content(text: str, remove_special_blocks: bool = None) -> str:
        """
        ç§»é™¤ç‰¹æ®Šå…§å®¹å€å¡Š - èˆ‡fine_tuning.pyé‚è¼¯å®Œå…¨ä¸€è‡´
        å¦‚æœ remove_special_blocks ç‚º Trueï¼Œå‰‡ç§»é™¤ç¨‹å¼ç¢¼ã€æ•¸å­¸å…¬å¼å’Œè¡¨æ ¼å€å¡Šã€‚
        é»˜èªï¼ˆNoneï¼‰ä¸ç§»é™¤ç‰¹æ®Šå…§å®¹ã€‚
        """
        if not remove_special_blocks or not isinstance(text, str):
            return text

        # ç§»é™¤ç¨‹å¼ç¢¼å€å¡Š
        text = re.sub(r'```.*?```', '[CODE_BLOCK]', text, flags=re.DOTALL)
        # ç§»é™¤æ•¸å­¸å…¬å¼å€å¡Š
        text = re.sub(r'\$\$.*?\$\$', '[MATH_BLOCK]', text, flags=re.DOTALL)
        # ç§»é™¤ Markdown è¡¨æ ¼
        text = re.sub(r'((?:\|.*\|[\r\n\s]*){2,})', '[TABLE_BLOCK]', text)
        return text

    @staticmethod
    def jaccard_similarity(text1: str, text2: str) -> float:
        """è¨ˆç®— Jaccard ç›¸ä¼¼åº¦ - èˆ‡fine_tuning.pyé‚è¼¯ä¸€è‡´"""
        if not isinstance(text1, str) or not isinstance(text2, str):
            return 0.0
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        return intersection / union if union > 0 else 0.0
    
    @staticmethod
    def count_code_blocks(text: str) -> int:
        """è¨ˆç®—markdownæ ¼å¼çš„ç¨‹å¼ç¢¼å€å¡Šæ•¸é‡ - èˆ‡fine_tuning.pyé‚è¼¯ä¸€è‡´"""
        if not isinstance(text, str):
            return 0
        return str(text).count('```') // 2

    @staticmethod
    def count_math_blocks(text: str) -> int:
        """è¨ˆç®—markdownæ ¼å¼çš„æ•¸å­¸å…¬å¼å€å¡Šæ•¸é‡ - èˆ‡fine_tuning.pyé‚è¼¯ä¸€è‡´"""
        if not isinstance(text, str):
            return 0
        math_pattern = re.compile(r'\\$\\$.*?\\$\\$', re.DOTALL)
        return len(math_pattern.findall(text))

    @staticmethod
    def count_table_blocks(text: str) -> int:
        """è¨ˆç®—markdownæ ¼å¼çš„è¡¨æ ¼æ•¸é‡ - èˆ‡fine_tuning.pyé‚è¼¯ä¸€è‡´"""
        if not isinstance(text, str):
            return 0
        table_pattern = re.compile(r'((?:\\|.*?\\|[\\r\\n\\s]*){2,})')
        return len(table_pattern.findall(text))
    
    @staticmethod
    def calculate_content_blocks_diff(text1_raw: str, text2_raw: str) -> int:
        """
        è¨ˆç®—markdownæ ¼å¼çš„ç¨‹å¼ç¢¼ã€æ•¸å­¸å…¬å¼å’Œè¡¨æ ¼å€å¡Šçš„ç¸½æ•¸é‡å·®å€¼ - èˆ‡fine_tuning.pyé‚è¼¯ä¸€è‡´
        
        Args:
            text1_raw (str): åŸå§‹ç¬¬ä¸€å€‹æ–‡æœ¬ (response_a)
            text2_raw (str): åŸå§‹ç¬¬äºŒå€‹æ–‡æœ¬ (response_b)
            
        Returns:
            int: å…§å®¹å€å¡Šç¸½æ•¸é‡å·®å€¼ (response_a - response_b)
        """
        count1_code = SimpleMetadataFeatures.count_code_blocks(text1_raw)
        count2_code = SimpleMetadataFeatures.count_code_blocks(text2_raw)
        code_diff = count1_code - count2_code

        count1_math = SimpleMetadataFeatures.count_math_blocks(text1_raw)
        count2_math = SimpleMetadataFeatures.count_math_blocks(text2_raw)
        math_diff = count1_math - count2_math

        count1_table = SimpleMetadataFeatures.count_table_blocks(text1_raw)
        count2_table = SimpleMetadataFeatures.count_table_blocks(text2_raw)
        table_diff = count1_table - count2_table
        
        return code_diff + math_diff + table_diff

    @staticmethod
    def count_special_blocks(text: str) -> int:
        """è¨ˆç®—ç‰¹æ®Šå€å¡Šæ•¸é‡ - èˆ‡fine_tuning.pyé‚è¼¯ä¸€è‡´ï¼ˆå·²å»¢æ£„ï¼Œæ”¹ç”¨calculate_content_blocks_diffï¼‰"""
        if not isinstance(text, str):
            return 0
        code_blocks = text.count('```') // 2
        math_blocks = len(re.findall(r'\$\$.*?\$\$', text, re.DOTALL))
        table_blocks = len(re.findall(r'((?:\|.*?\|[\r\n\s]*){2,})', text))
        return code_blocks + math_blocks + table_blocks
    
    @staticmethod
    def calculate_ttr(text: str) -> float:
        """è¨ˆç®—è©å½™è±å¯Œåº¦ (TTR) - èˆ‡fine_tuning.pyé‚è¼¯ä¸€è‡´"""
        if not isinstance(text, str) or len(text.strip()) == 0:
            return 0.0
        words = text.lower().split()
        if len(words) == 0:
            return 0.0
        return len(set(words)) / len(words)
    
    @staticmethod
    def standardize_features(features_dict: Dict[str, float]) -> Dict[str, float]:
        """ä½¿ç”¨è¼‰å…¥çš„çµ±è¨ˆåƒæ•¸é€²è¡Œz-scoreæ¨™æº–åŒ–"""
        if SimpleMetadataFeatures.FEATURE_STATS is None:
            raise ValueError("å°šæœªè¼‰å…¥metadataçµ±è¨ˆåƒæ•¸ï¼Œè«‹å…ˆèª¿ç”¨load_metadata_stats()")
        
        standardized = {}
        for feature_name, value in features_dict.items():
            if feature_name in SimpleMetadataFeatures.FEATURE_STATS:
                mean = SimpleMetadataFeatures.FEATURE_STATS[feature_name]['mean']
                std = SimpleMetadataFeatures.FEATURE_STATS[feature_name]['std']
                standardized[feature_name] = (value - mean) / std if std > 0 else 0.0
            else:
                standardized[feature_name] = value
        return standardized
    
    @staticmethod
    def extract_features(row: pd.Series) -> Dict[str, float]:
        """
        æå–æ ¸å¿ƒ5ç¶­å…ƒæ•¸æ“šç‰¹å¾µ - å®Œå…¨åŒ¹é…fine_tuning.pyçš„ç‰¹å¾µè™•ç†é‚è¼¯
        
        é—œéµè¦é»ï¼š
        1. jaccard_index: response_a vs response_bï¼ˆä½¿ç”¨è™•ç†éçš„æ–‡æœ¬ï¼‰
        2. ttr_diff: response_a - response_bï¼ˆåŸå§‹å·®å€¼ï¼Œä¸å–çµ•å°å€¼ï¼‰
        3. ttr_ratio: max(ttr_a, ttr_b) / max(min(ttr_a, ttr_b), 0.001) ï¼ˆmax/minå½¢å¼ï¼Œèˆ‡è¨“ç·´å®Œå…¨ä¸€è‡´ï¼‰
        4. å…¶ä»–ç‰¹å¾µä¿æŒä¸€è‡´çš„logç¸®æ”¾å’Œè™•ç†é‚è¼¯
        """
        response_a_raw = str(row['response_a'])
        response_b_raw = str(row['response_b'])
        
        # è™•ç†éçš„æ–‡æœ¬ï¼ˆç”¨æ–¼ jaccard, ttr, ttr_ratioï¼‰
        response_a_for_jaccard_ttr = SimpleMetadataFeatures.remove_special_content(response_a_raw)
        response_b_for_jaccard_ttr = SimpleMetadataFeatures.remove_special_content(response_b_raw)
        
        # TTRè¨ˆç®—
        ttr_a = SimpleMetadataFeatures.calculate_ttr(response_a_for_jaccard_ttr)
        ttr_b = SimpleMetadataFeatures.calculate_ttr(response_b_for_jaccard_ttr)
        ttr_diff = ttr_a - ttr_b  # åŸå§‹å·®å€¼ï¼ˆä¸å–çµ•å°å€¼ï¼‰
        
        # TTRæ¯”å€¼è¨ˆç®— - èˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´ï¼šmax/min å½¢å¼
        ttr_ratio = max(ttr_a, ttr_b) / max(min(ttr_a, ttr_b), 0.001) if min(ttr_a, ttr_b) > 0 else 1.0
        
        # åŸå§‹å·®ç•°å€¼ï¼ˆåœ¨åŸå§‹æ–‡æœ¬ä¸Šè¨ˆç®—ï¼‰
        length_diff = len(response_a_raw) - len(response_b_raw)
        
        # ç‰¹æ®Šå€å¡Šå·®ç•° - ä½¿ç”¨èˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´çš„é‚è¼¯
        content_blocks_diff_raw = SimpleMetadataFeatures.calculate_content_blocks_diff(response_a_raw, response_b_raw)
        
        # å°æ•¸ç¸®æ”¾ï¼ˆä¿ç•™ç¬¦è™Ÿï¼‰
        scaled_length_diff = np.sign(length_diff) * np.log1p(abs(length_diff))
        scaled_content_blocks_diff = np.sign(content_blocks_diff_raw) * np.log1p(abs(content_blocks_diff_raw))
        
        # æ”¶é›†åŸå§‹ç‰¹å¾µå€¼ï¼ˆèˆ‡fine_tuning.pyå®Œå…¨ä¸€è‡´ï¼‰
        features_dict = {
            'jaccard_index': SimpleMetadataFeatures.jaccard_similarity(
                response_a_for_jaccard_ttr, response_b_for_jaccard_ttr),
            'ttr_diff': ttr_diff,
            'ttr_ratio': ttr_ratio,
            'content_blocks_diff': scaled_content_blocks_diff,
            'length_diff': scaled_length_diff
        }
        
        # ç›´æ¥è¿”å›åŸå§‹ç‰¹å¾µå€¼ï¼ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼Œä¸æ‡‰ç”¨æ¨™æº–åŒ–ï¼‰
        return features_dict

# --------------------------------------------------------------------------
# 3. Test Dataset Class
# --------------------------------------------------------------------------
class KaggleTestDataset(Dataset):
    """
    Kaggle æ¸¬è©¦æ•¸æ“šé›† - dual-tower æ¨¡å‹å°ˆç”¨
    å®Œå…¨åŒ¹é… fine_tuning.py ä¸­çš„ DualTowerPairDataset è¼¸å…¥æ ¼å¼
    """
    
    def __init__(self, dataframe: pd.DataFrame, tokenizer, config: KaggleConfig):
        self.df = dataframe
        self.tokenizer = tokenizer
        self.config = config
        
        print(f"  - å‰µå»ºæ¸¬è©¦æ•¸æ“šé›†ï¼Œæ¨£æœ¬æ•¸: {len(self.df)}")
        print(f"  - æ¨¡å‹æ¶æ§‹: dual-tower")
        
        # æå–å…ƒæ•¸æ“šç‰¹å¾µ
        print("  - æå–å…ƒæ•¸æ“šç‰¹å¾µ...")
        tqdm.pandas(desc="Extracting features")
        # ä½¿ç”¨èˆ‡è¨“ç·´å®Œå…¨ç›¸åŒçš„æå–é‚è¼¯ (å„ªå…ˆä½¿ç”¨å¤–éƒ¨æ¨¡çµ„ï¼Œå¦å‰‡é€€å›å…§å»º SimpleMetadataFeatures)
        if MetadataFeatures is not None:
            self.features = self.df.progress_apply(MetadataFeatures.extract_core_features, axis=1)
        else:
            self.features = self.df.progress_apply(SimpleMetadataFeatures.extract_features, axis=1)
        
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        metadata = self.features.iloc[idx]
        
        # å…ˆåš z-score æ¨™æº–åŒ–ï¼ˆä¾ç…§ SimpleMetadataFeatures.FEATURE_STATSï¼‰
        metadata_std = SimpleMetadataFeatures.standardize_features(metadata)
        
        # ç”Ÿæˆèˆ‡è¨“ç·´ç›¸åŒé †åºçš„ç‰¹å¾µå‘é‡
        if MetadataFeatures is not None:
            feature_order = MetadataFeatures.get_feature_columns('core')
            metadata_values = [float(metadata_std[col]) for col in feature_order]
        else:
            # Fallback: ä½¿ç”¨å›ºå®šé †åº
            metadata_values = [
                float(metadata_std['jaccard_index']),
                float(metadata_std['ttr_diff']),
                float(metadata_std['ttr_ratio']),
                float(metadata_std['content_blocks_diff']),
                float(metadata_std['length_diff'])
            ]
        metadata_tensor = torch.tensor(metadata_values, dtype=torch.float32)
        
        # dual-tower æ¨¡å‹ï¼šä½¿ç”¨åˆ†é›¢è¼¸å…¥ï¼ˆèˆ‡ fine_tuning.py è¨“ç·´æ™‚ä¸€è‡´ï¼‰
        prompt = str(row['prompt'])
        response_a = str(row['response_a'])
        response_b = str(row['response_b'])
        
        # èˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼šåœ¨ Tokenize å‰ç§»é™¤ç‰¹æ®Šå€å¡Š
        prompt_clean = SimpleMetadataFeatures.remove_special_content(prompt, remove_special_blocks=True)
        response_a_clean = SimpleMetadataFeatures.remove_special_content(response_a, remove_special_blocks=True)
        response_b_clean = SimpleMetadataFeatures.remove_special_content(response_b, remove_special_blocks=True)

        p_encoded = self.tokenizer(
            prompt_clean,
            add_special_tokens=True,
            max_length=self.config.MAX_LENGTH,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        a_encoded = self.tokenizer(
            response_a_clean,
            add_special_tokens=True,
            max_length=self.config.MAX_LENGTH,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        b_encoded = self.tokenizer(
            response_b_clean,
            add_special_tokens=True,
            max_length=self.config.MAX_LENGTH,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'p_input_ids': p_encoded['input_ids'].squeeze(),
            'p_attention_mask': p_encoded['attention_mask'].squeeze(),
            'a_input_ids': a_encoded['input_ids'].squeeze(),
            'a_attention_mask': a_encoded['attention_mask'].squeeze(),
            'b_input_ids': b_encoded['input_ids'].squeeze(),
            'b_attention_mask': b_encoded['attention_mask'].squeeze(),
            'metadata_features': metadata_tensor
        }

# --------------------------------------------------------------------------
# 4. Model Inference Class
# --------------------------------------------------------------------------
class KaggleInference:
    """Kaggle æ¨ç†é¡"""
    
    def __init__(self, config: KaggleConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        
        self.load_model()
    
    def load_model(self):
        """è¼‰å…¥é è¨“ç·´æ¨¡å‹"""
        print(f"\nè¼‰å…¥é è¨“ç·´æ¨¡å‹...")
        
        try:
            # æª¢æŸ¥æ¨¡å‹è·¯å¾‘
            if not os.path.exists(self.config.MODEL_PATH):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾‘: {self.config.MODEL_PATH}")
            
            # è¼‰å…¥metadataçµ±è¨ˆåƒæ•¸ï¼ˆå¿…é ˆåœ¨å‰µå»ºdatasetä¹‹å‰ï¼‰
            print("  - è¼‰å…¥metadataçµ±è¨ˆåƒæ•¸...")
            SimpleMetadataFeatures.load_metadata_stats(self.config.MODEL_PATH)
            
            # è¼‰å…¥åŸºæœ¬æ¨¡å‹ä¿¡æ¯
            metrics_path = os.path.join(self.config.MODEL_PATH, 'metrics.json')
            if os.path.exists(metrics_path):
                with open(metrics_path, 'r') as f:
                    metrics = json.load(f)
                
                print(f"  - æ¨¡å‹æ¶æ§‹: {metrics.get('model_arch', 'dual')}")
                print(f"  - è¨“ç·´æ™‚é–“: {metrics.get('timestamp', 'Unknown')}")
                print(f"  - é©—è­‰æ€§èƒ½:")
                print(f"    * Log Loss: {metrics.get('log_loss', 'N/A'):.6f}")
                print(f"    * Accuracy: {metrics.get('accuracy', 'N/A'):.4f}")
                
                # é¡¯ç¤ºä¸»è¦è¶…åƒæ•¸
                hyperparams = metrics.get('hyperparams', {})
                if hyperparams:
                    print(f"  - è¨“ç·´é…ç½®:")
                    print(f"    * Epochs: {hyperparams.get('epochs', 'N/A')}")
                    print(f"    * Learning Rate: {hyperparams.get('learning_rate', 'N/A')}")
                    print(f"    * Batch Size: {hyperparams.get('batch_size', 'N/A')}")
                
                # é¡¯ç¤ºpreprocessingé…ç½®
                preprocessing_config = metrics.get('preprocessing_config', {})
                if preprocessing_config:
                    print(f"  - é è™•ç†é…ç½®:")
                    print(f"    * Extract Metadata: {preprocessing_config.get('extract_metadata', 'N/A')}")
                    print(f"    * Metadata Type: {preprocessing_config.get('metadata_type', 'N/A')}")
            
            # è¼‰å…¥ tokenizer
            print("  - è¼‰å…¥ tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.MODEL_PATH, 
                local_files_only=True
            )
            
            # è¼‰å…¥ dual-tower æ¨¡å‹
            print(f"  - è¼‰å…¥ dual-tower æ¶æ§‹æ¨¡å‹...")
            self.model = DualTowerPairClassifier.from_pretrained(
                self.config.MODEL_PATH,
                local_files_only=True
            )
            
            # ç§»å‹•åˆ°è¨­å‚™
            self.model.to(self.config.DEVICE)
            self.model.eval()
            
            print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨è¨­å‚™: {self.config.DEVICE}")
            print(f"  - é‡è¦ï¼šdual-tower æ¨¡å‹ä½¿ç”¨åˆ†é›¢è¼¸å…¥æ ¼å¼ï¼ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
    
    def predict(self, test_df: pd.DataFrame) -> pd.DataFrame:
        """å°æ¸¬è©¦æ•¸æ“šé€²è¡Œé æ¸¬"""
        print(f"\né–‹å§‹é æ¸¬...")
        
        # å‰µå»ºæ•¸æ“šé›†
        test_dataset = KaggleTestDataset(test_df, self.tokenizer, self.config)
        
        # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.BATCH_SIZE,
            shuffle=False,
            num_workers=0  # Kaggle ç’°å¢ƒå»ºè­°è¨­ç‚º 0
        )
        
        # é æ¸¬
        all_predictions = []
        
        print(f"  - é–‹å§‹æ‰¹æ¬¡é æ¸¬ï¼Œæ‰¹æ¬¡å¤§å°: {self.config.BATCH_SIZE}")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(tqdm(test_loader, desc="Predicting")):
                # dual-tower æ¨¡å‹ï¼šä½¿ç”¨åˆ†é›¢è¼¸å…¥
                p_input_ids = batch['p_input_ids'].to(self.config.DEVICE)
                p_attention_mask = batch['p_attention_mask'].to(self.config.DEVICE)
                a_input_ids = batch['a_input_ids'].to(self.config.DEVICE)
                a_attention_mask = batch['a_attention_mask'].to(self.config.DEVICE)
                b_input_ids = batch['b_input_ids'].to(self.config.DEVICE)
                b_attention_mask = batch['b_attention_mask'].to(self.config.DEVICE)
                metadata_features = batch['metadata_features'].to(self.config.DEVICE)
                
                outputs = self.model(
                    p_input_ids=p_input_ids,
                    p_attention_mask=p_attention_mask,
                    a_input_ids=a_input_ids,
                    a_attention_mask=a_attention_mask,
                    b_input_ids=b_input_ids,
                    b_attention_mask=b_attention_mask,
                    metadata_features=metadata_features
                )
                
                logits = outputs.logits if hasattr(outputs, 'logits') else outputs['logits']
                
                # è¨ˆç®—æ¦‚ç‡
                probabilities = torch.nn.functional.softmax(logits, dim=-1)
                all_predictions.append(probabilities.cpu().numpy())
                
                # æ¸…ç†è¨˜æ†¶é«”
                if (batch_idx + 1) % 10 == 0:
                    torch.cuda.empty_cache()
        
        # åˆä½µæ‰€æœ‰é æ¸¬çµæœ
        predictions = np.vstack(all_predictions)
        
        # å‰µå»ºæäº¤æ–‡ä»¶
        submission = pd.DataFrame({
            'id': test_df['id'].values,
            'winner_model_a': predictions[:, 0],
            'winner_model_b': predictions[:, 1],
            'winner_tie': predictions[:, 2]
        })
        
        # ç¢ºä¿æ¦‚ç‡ç¸½å’Œç‚º 1
        row_sums = submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1)
        submission['winner_model_a'] /= row_sums
        submission['winner_model_b'] /= row_sums
        submission['winner_tie'] /= row_sums
        
        print(f"âœ… é æ¸¬å®Œæˆï¼Œå…±è™•ç† {len(submission)} å€‹æ¨£æœ¬")
        
        # é¡¯ç¤ºé æ¸¬åˆ†å¸ƒ
        final_preds = np.argmax(predictions, axis=1)
        print(f"\né æ¸¬åˆ†å¸ƒ:")
        print(f"  - Model A å‹åˆ©: {np.sum(final_preds == 0)} ({np.sum(final_preds == 0)/len(final_preds)*100:.1f}%)")
        print(f"  - Model B å‹åˆ©: {np.sum(final_preds == 1)} ({np.sum(final_preds == 1)/len(final_preds)*100:.1f}%)")
        print(f"  - å¹³å±€: {np.sum(final_preds == 2)} ({np.sum(final_preds == 2)/len(final_preds)*100:.1f}%)")
        
        return submission

# --------------------------------------------------------------------------
# 5. Main Execution
# --------------------------------------------------------------------------
def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("=" * 60)
    print("Kaggle é›¢ç·šæ¨ç†è…³æœ¬ - kaggle_last.py")
    print("å®Œå…¨å…¼å®¹ fine_tuning.py ä¸­çš„ dual-tower æ¨¡å‹")
    print("=" * 60)
    
    try:
        # 1. åˆå§‹åŒ–é…ç½®
        config = KaggleConfig()
        
        # 2. è¼‰å…¥æ¸¬è©¦æ•¸æ“š
        print(f"\nè¼‰å…¥æ¸¬è©¦æ•¸æ“š...")
        test_df = pd.read_csv(config.TEST_PATH)
        
        # å¦‚æœä½¿ç”¨è¨“ç·´æ•¸æ“šé€²è¡Œæ¸¬è©¦ï¼Œåªå–å‰100è¡Œä¸¦å‰µå»ºidæ¬„ä½
        if config.TEST_PATH.endswith('train.csv'):
            print(f"  - ä½¿ç”¨è¨“ç·´æ•¸æ“šé€²è¡ŒåŠŸèƒ½é©—è­‰ï¼Œåªè™•ç†å‰100è¡Œ")
            test_df = test_df.head(100).copy()
            # å‰µå»ºidæ¬„ä½ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'id' not in test_df.columns:
                test_df['id'] = range(len(test_df))
            print(f"  - å‰µå»ºäº† {len(test_df)} å€‹æ¸¬è©¦æ¨£æœ¬")
        
        print(f"  - æ¸¬è©¦æ•¸æ“šå½¢ç‹€: {test_df.shape}")
        print(f"  - æ¬„ä½: {list(test_df.columns)}")
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_columns = ['id', 'prompt', 'response_a', 'response_b']
        missing_columns = [col for col in required_columns if col not in test_df.columns]
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
        
        # 3. åˆå§‹åŒ–æ¨ç†å™¨
        inference = KaggleInference(config)
        
        # 4. é€²è¡Œé æ¸¬
        submission = inference.predict(test_df)
        
        # 5. ä¿å­˜çµæœ
        print(f"\nä¿å­˜æäº¤æ–‡ä»¶...")
        submission.to_csv(config.OUTPUT_PATH, index=False)
        print(f"âœ… æäº¤æ–‡ä»¶å·²ä¿å­˜: {config.OUTPUT_PATH}")
        
        # 6. é¡¯ç¤ºæäº¤æ–‡ä»¶é è¦½
        print(f"\næäº¤æ–‡ä»¶é è¦½:")
        print(submission.head(10))
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Kaggle é›¢ç·šæ¨ç†å®Œæˆï¼")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main() 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import os

def analyze_dataset_distribution(csv_file_path):
    """
    åˆ†æè³‡æ–™é›†çš„åˆ†é¡åˆ†ä½ˆæƒ…æ³
    
    Args:
        csv_file_path (str): CSVæª”æ¡ˆè·¯å¾‘
    """
    print("="*60)
    print("è³‡æ–™é›†åˆ†æå ±å‘Š")
    print("="*60)
    
    try:
        # è®€å–è³‡æ–™é›†
        print("æ­£åœ¨è®€å–è³‡æ–™é›†...")
        df = pd.read_csv(csv_file_path)
        
        # åŸºæœ¬è³‡æ–™é›†è³‡è¨Š
        print(f"\nğŸ“Š åŸºæœ¬è³‡æ–™é›†è³‡è¨Š:")
        print(f"è³‡æ–™ç¸½ç­†æ•¸: {len(df):,}")
        print(f"æ¬„ä½æ•¸é‡: {len(df.columns)}")
        print(f"æ¬„ä½åç¨±: {list(df.columns)}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰targetæ¬„ä½ï¼ˆå¸¸è¦‹çš„ç›®æ¨™è®Šæ•¸åç¨±ï¼‰
        possible_target_columns = ['winner', 'label', 'target', 'class', 'winner_model_a', 'winner_model_b', 'tie']
        target_column = None
        
        for col in possible_target_columns:
            if col in df.columns:
                target_column = col
                break
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ˜ç¢ºçš„ç›®æ¨™æ¬„ä½ï¼Œé¡¯ç¤ºæ‰€æœ‰æ¬„ä½çš„å”¯ä¸€å€¼
        if target_column is None:
            print(f"\nğŸ” å„æ¬„ä½çš„å”¯ä¸€å€¼æ•¸é‡:")
            for col in df.columns:
                unique_count = df[col].nunique()
                print(f"  {col}: {unique_count} å€‹å”¯ä¸€å€¼")
                
                # å¦‚æœå”¯ä¸€å€¼è¼ƒå°‘ï¼Œé¡¯ç¤ºå…·é«”çš„å€¼
                if unique_count <= 10:
                    unique_values = df[col].unique()
                    print(f"    å€¼: {unique_values}")
        
        # å¦‚æœæ‰¾åˆ°ç›®æ¨™æ¬„ä½ï¼Œé€²è¡Œè©³ç´°åˆ†æ
        if target_column:
            print(f"\nğŸ¯ ç›®æ¨™è®Šæ•¸åˆ†æ (æ¬„ä½: {target_column}):")
            value_counts = df[target_column].value_counts()
            percentages = df[target_column].value_counts(normalize=True) * 100
            
            print(f"\né¡åˆ¥åˆ†ä½ˆ:")
            for category, count in value_counts.items():
                percentage = percentages[category]
                print(f"  {category}: {count:,} ç­† ({percentage:.2f}%)")
            
            # æª¢æŸ¥é¡åˆ¥å¹³è¡¡æ€§
            max_count = value_counts.max()
            min_count = value_counts.min()
            imbalance_ratio = max_count / min_count
            
            print(f"\nâš–ï¸ é¡åˆ¥å¹³è¡¡æ€§åˆ†æ:")
            print(f"æœ€å¤šé¡åˆ¥: {max_count:,} ç­†")
            print(f"æœ€å°‘é¡åˆ¥: {min_count:,} ç­†")
            print(f"ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
            
            if imbalance_ratio > 2:
                print("âš ï¸  è³‡æ–™é›†å­˜åœ¨é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ")
            else:
                print("âœ… è³‡æ–™é›†é¡åˆ¥åˆ†ä½ˆç›¸å°å¹³è¡¡")
            
            # ç¹ªè£½åˆ†ä½ˆåœ–
            plt.figure(figsize=(12, 8))
            
            # é•·æ¢åœ–
            plt.subplot(2, 2, 1)
            value_counts.plot(kind='bar', color='skyblue', edgecolor='black')
            plt.title('é¡åˆ¥åˆ†ä½ˆ - çµ•å°æ•¸é‡')
            plt.xlabel('é¡åˆ¥')
            plt.ylabel('æ•¸é‡')
            plt.xticks(rotation=45)
            
            # åœ“é¤…åœ–
            plt.subplot(2, 2, 2)
            plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%', startangle=90)
            plt.title('é¡åˆ¥åˆ†ä½ˆ - ç™¾åˆ†æ¯”')
            
            # ç™¾åˆ†æ¯”é•·æ¢åœ–
            plt.subplot(2, 2, 3)
            percentages.plot(kind='bar', color='lightcoral', edgecolor='black')
            plt.title('é¡åˆ¥åˆ†ä½ˆ - ç™¾åˆ†æ¯”')
            plt.xlabel('é¡åˆ¥')
            plt.ylabel('ç™¾åˆ†æ¯” (%)')
            plt.xticks(rotation=45)
            
            # ç´¯ç©åˆ†ä½ˆåœ–
            plt.subplot(2, 2, 4)
            cumulative_percentages = percentages.cumsum()
            cumulative_percentages.plot(kind='line', marker='o', color='green', linewidth=2, markersize=6)
            plt.title('ç´¯ç©åˆ†ä½ˆ')
            plt.xlabel('é¡åˆ¥')
            plt.ylabel('ç´¯ç©ç™¾åˆ†æ¯” (%)')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # å„²å­˜åœ–è¡¨
            output_dir = 'results'
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            plt.savefig(f'{output_dir}/dataset_distribution_analysis.png', dpi=300, bbox_inches='tight')
            print(f"\nğŸ“ˆ åœ–è¡¨å·²å„²å­˜è‡³: {output_dir}/dataset_distribution_analysis.png")
            plt.show()
        
        # ç¼ºå¤±å€¼åˆ†æ
        print(f"\nğŸ” ç¼ºå¤±å€¼åˆ†æ:")
        missing_data = df.isnull().sum()
        if missing_data.sum() == 0:
            print("âœ… æ²’æœ‰ç™¼ç¾ç¼ºå¤±å€¼")
        else:
            print("ç™¼ç¾ç¼ºå¤±å€¼:")
            for col, missing_count in missing_data[missing_data > 0].items():
                missing_percentage = (missing_count / len(df)) * 100
                print(f"  {col}: {missing_count} å€‹ç¼ºå¤±å€¼ ({missing_percentage:.2f}%)")
        
        # è³‡æ–™å‹æ…‹åˆ†æ
        print(f"\nğŸ“‹ è³‡æ–™å‹æ…‹åˆ†æ:")
        data_types = df.dtypes.value_counts()
        for dtype, count in data_types.items():
            print(f"  {dtype}: {count} å€‹æ¬„ä½")
        
        # å„²å­˜åˆ†æçµæœ
        analysis_results = {
            'ç¸½ç­†æ•¸': len(df),
            'æ¬„ä½æ•¸é‡': len(df.columns),
            'æ¬„ä½åç¨±': list(df.columns),
            'ç¼ºå¤±å€¼ç¸½æ•¸': missing_data.sum(),
            'è³‡æ–™å‹æ…‹': dict(data_types)
        }
        
        if target_column:
            analysis_results['ç›®æ¨™è®Šæ•¸'] = target_column
            analysis_results['é¡åˆ¥åˆ†ä½ˆ'] = dict(value_counts)
            analysis_results['é¡åˆ¥ç™¾åˆ†æ¯”'] = dict(percentages)
            analysis_results['ä¸å¹³è¡¡æ¯”ä¾‹'] = imbalance_ratio
        
        # å„²å­˜åˆ°æª”æ¡ˆ
        import json
        with open(f'{output_dir}/dataset_analysis_results.json', 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åˆ†æçµæœå·²å„²å­˜è‡³: {output_dir}/dataset_analysis_results.json")
        
    except Exception as e:
        print(f"âŒ è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        print("è«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘æ˜¯å¦æ­£ç¢ºï¼Œæˆ–æª”æ¡ˆæ˜¯å¦å¤ªå¤§")

def analyze_specific_columns(csv_file_path, columns_to_analyze=None):
    """
    åˆ†æç‰¹å®šæ¬„ä½çš„åˆ†ä½ˆ
    
    Args:
        csv_file_path (str): CSVæª”æ¡ˆè·¯å¾‘
        columns_to_analyze (list): è¦åˆ†æçš„æ¬„ä½åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨ chunksize ä¾†è™•ç†å¤§æª”æ¡ˆ
        print("æ­£åœ¨åˆ†æå¤§å‹æª”æ¡ˆ...")
        
        if columns_to_analyze is None:
            # å…ˆè®€å–å°‘é‡è³‡æ–™ä¾†ç¢ºå®šæ¬„ä½
            sample_df = pd.read_csv(csv_file_path, nrows=1000)
            print(f"æ¨£æœ¬è³‡æ–™æ¬„ä½: {list(sample_df.columns)}")
            return
        
        # åˆ†å¡Šè®€å–ä¸¦çµ±è¨ˆ
        chunk_size = 10000
        value_counts = {}
        total_rows = 0
        
        for chunk in pd.read_csv(csv_file_path, chunksize=chunk_size):
            total_rows += len(chunk)
            
            for col in columns_to_analyze:
                if col in chunk.columns:
                    if col not in value_counts:
                        value_counts[col] = Counter()
                    
                    # çµ±è¨ˆç•¶å‰chunkçš„å€¼
                    chunk_counts = chunk[col].value_counts()
                    for value, count in chunk_counts.items():
                        value_counts[col][value] += count
            
            print(f"å·²è™•ç† {total_rows:,} ç­†è³‡æ–™...")
        
        # é¡¯ç¤ºçµæœ
        print(f"\nğŸ“Š ç¸½å…±è™•ç†äº† {total_rows:,} ç­†è³‡æ–™")
        
        for col, counts in value_counts.items():
            print(f"\nğŸ¯ æ¬„ä½ '{col}' çš„åˆ†ä½ˆ:")
            total_count = sum(counts.values())
            
            for value, count in counts.most_common():
                percentage = (count / total_count) * 100
                print(f"  {value}: {count:,} ç­† ({percentage:.2f}%)")
        
    except Exception as e:
        print(f"âŒ åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    # è¨­å®šæª”æ¡ˆè·¯å¾‘
    train_file = "train.csv"
    
    print("ğŸš€ é–‹å§‹åˆ†æè³‡æ–™é›†...")
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(train_file):
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {train_file}")
        print("è«‹ç¢ºèªæª”æ¡ˆæ˜¯å¦åœ¨ç•¶å‰ç›®éŒ„ä¸­")
    else:
        file_size = os.path.getsize(train_file) / (1024 * 1024)  # MB
        print(f"ğŸ“ æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
        
        if file_size > 100:  # å¦‚æœæª”æ¡ˆå¤§æ–¼100MB
            print("ğŸ“‹ æª”æ¡ˆè¼ƒå¤§ï¼Œå…ˆæŸ¥çœ‹æ¬„ä½çµæ§‹...")
            analyze_specific_columns(train_file)
              # æ ¹æ“šç™¼ç¾çš„æ¬„ä½çµæ§‹ï¼Œåˆ†ææ¨¡å‹æ¯”è¼ƒçš„åˆ†é¡æ¬„ä½
            classification_cols = ['winner_model_a', 'winner_model_b', 'winner_tie']
            print(f"\nğŸ¯ åˆ†ææ¨¡å‹å‹è² åˆ†é¡æ¬„ä½: {classification_cols}")
            analyze_specific_columns(train_file, classification_cols)
        else:
            # ç›´æ¥åˆ†ææ•´å€‹æª”æ¡ˆ
            analyze_dataset_distribution(train_file)

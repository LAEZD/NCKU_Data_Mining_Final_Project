{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cca1fd7",
   "metadata": {},
   "source": [
    " Cell 1: 專案說明與環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da537ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 1: 專案說明與環境設定\n",
    "# ==============================================================================\n",
    "#\n",
    "# 專案目標：\n",
    "# 驗證 '語義相似度' (Semantic Similarity) 作為一個機器學習特徵，\n",
    "# 是否比 '字元級 Jaccard 相似度' (Character-level Jaccard Similarity) \n",
    "# 更能有效地區分不同勝負標籤 (A勝/B勝/平手) 的數據。\n",
    "#\n",
    "# 工作流程：\n",
    "# 1. (Cell 2) 加載 train.csv 數據，並預先計算 'label' 和 'resp_jaccard' 等基礎欄位。\n",
    "# 2. (Cell 3) 從本地加載 GloVe 詞向量模型，並定義計算語義相似度的核心函數。\n",
    "# 3. (Cell 4) 將語義相似度函數應用於數據，生成新的 'semantic_similarity' 特徵。\n",
    "# 4. (Cell 5) 透過視覺化圖表與統計數據，並排對比兩種相似度特徵的有效性。\n",
    "#\n",
    "# --- 載入所有必要的函式庫 ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- Matplotlib 中文與負號顯示設定 (美化圖表) ---\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Heiti TC', 'sans-serif'] # 優先使用微軟正黑體\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# --- tqdm 與 pandas 整合 ---\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\"--- Cell 1: 環境設定完畢 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f289b",
   "metadata": {},
   "source": [
    "Cell 2: 數據加載與基礎特徵準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 2: 數據加載與基礎特徵準備\n",
    "# 說明：此 Cell 負責加載您的數據，並創建後續分析所需的 'df' DataFrame\n",
    "# 和 'label'、'resp_jaccard' 欄位。請務必先執行此 Cell。\n",
    "# ==============================================================================\n",
    "print(\"--- Cell 2: 開始加載和準備基礎數據 ---\")\n",
    "\n",
    "try:\n",
    "    # 從 CSV 文件加載\n",
    "    file_path = 'train.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"找不到數據文件: {file_path}\")\n",
    "        \n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"數據 '{file_path}' 加載成功，形狀為: {df.shape}\")\n",
    "\n",
    "    # --- 確保後續分析所需的欄位存在 ---\n",
    "\n",
    "    # 1. 創建 'label' 欄位 (0: A勝, 1: B勝, 2: 平手)\n",
    "    if 'label' not in df.columns:\n",
    "        print(\"正在創建 'label' 欄位...\")\n",
    "        def get_label(row):\n",
    "            if row['winner_model_a'] == 1: return 0\n",
    "            if row['winner_model_b'] == 1: return 1\n",
    "            return 2\n",
    "        df['label'] = df.apply(get_label, axis=1)\n",
    "\n",
    "    # 2. 創建 'resp_jaccard' 欄位以供對比\n",
    "    if 'resp_jaccard' not in df.columns:\n",
    "        print(\"正在創建 'resp_jaccard' 欄位...\")\n",
    "        def get_char_jaccard_similarity(text1, text2):\n",
    "            set1, set2 = set(str(text1)), set(str(text2))\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            return intersection / union if union != 0 else 0\n",
    "        df['resp_jaccard'] = df.progress_apply(lambda row: get_char_jaccard_similarity(row['response_a'], row['response_b']), axis=1)\n",
    "\n",
    "    print(\"\\n'df' 已成功創建並準備就緒，包含 'label' 和 'resp_jaccard' 欄位。\")\n",
    "    # 顯示 DataFrame 的前幾行以作確認\n",
    "    display(df.head(3))\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n錯誤：{e}\\n請確認 'train.csv' 檔案與您的筆記本在同一個資料夾中。\")\n",
    "except Exception as e:\n",
    "    print(f\"發生未知錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193eb415",
   "metadata": {},
   "source": [
    "Cell 3: GloVe 模型加載與核心函數定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03978e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 3: GloVe 模型加載與核心函數定義\n",
    "# 說明：此 Cell 從本地加載 GloVe 模型並定義相關計算函數。\n",
    "# ==============================================================================\n",
    "print(\"--- Cell 3: 準備 GloVe 模型與相關工具 ---\")\n",
    "\n",
    "# --- 定義本地文件路徑並進行檢查 ---\n",
    "glove_file_path = 'glove.6B.100d.txt'\n",
    "\n",
    "if not os.path.exists(glove_file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"錯誤：在當前目錄下找不到 '{glove_file_path}'。\\n\"\n",
    "        \"請確認您已手動下載該文件，並將其與您的 .ipynb 文件放在同一個資料夾中。\"\n",
    "    )\n",
    "print(f\"成功找到本地 GloVe 文件: '{glove_file_path}'\")\n",
    "\n",
    "# --- 準備 NLTK 工具 (停用詞、分詞器) ---\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"正在下載 NLTK 依賴項 (stopwords, punkt)...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"NLTK 依賴項準備完畢。\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- 定義核心計算函數 ---\n",
    "def load_glove_embeddings(file_path):\n",
    "    print(f\"正在從 '{file_path}' 加載 GloVe 詞向量...\")\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"加載詞向量\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            except ValueError:\n",
    "                pass\n",
    "    print(f\"成功加載 {len(embeddings_index)} 個詞向量。\")\n",
    "    return embeddings_index\n",
    "\n",
    "def sentence_to_vector(sentence, embeddings, dim=100):\n",
    "    if not isinstance(sentence, str): return np.zeros(dim)\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    valid_words = [word for word in words if word.isalpha() and word not in stop_words and word in embeddings]\n",
    "    if not valid_words: return np.zeros(dim)\n",
    "    return np.mean([embeddings[word] for word in valid_words], axis=0)\n",
    "\n",
    "def calculate_semantic_similarity(text1, text2, embeddings):\n",
    "    vec1 = sentence_to_vector(text1, embeddings)\n",
    "    vec2 = sentence_to_vector(text2, embeddings)\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0: return 0.0\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# --- 將 GloVe 模型加載到內存 ---\n",
    "embeddings_index = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "print(\"\\nGloVe 模型已加載，核心函數已定義完畢。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442419b",
   "metadata": {},
   "source": [
    "Cell 4: 計算語義相似度特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 4: 計算語義相似度特徵\n",
    "# 說明：此 Cell 使用上一步加載的模型來為 DataFrame 計算新的特徵欄位。\n",
    "# ==============================================================================\n",
    "print(\"--- Cell 4: 開始計算 'semantic_similarity' 特徵 ---\")\n",
    "\n",
    "# 檢查 'df' 是否存在，確保執行順序正確\n",
    "if 'df' not in locals() or df.empty:\n",
    "    raise NameError(\"錯誤：找不到 DataFrame 'df'。請先執行 Cell 2。\")\n",
    "\n",
    "if 'semantic_similarity' not in df.columns:\n",
    "    df['semantic_similarity'] = df.progress_apply(\n",
    "        lambda row: calculate_semantic_similarity(row['response_a'], row['response_b'], embeddings_index),\n",
    "        axis=1\n",
    "    )\n",
    "    print(\"\\n'semantic_similarity' 特徵計算完成。\")\n",
    "else:\n",
    "    print(\"\\n'semantic_similarity' 特徵已存在，跳過計算。\")\n",
    "\n",
    "display(df[['response_a', 'response_b', 'resp_jaccard', 'semantic_similarity']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1523e2c",
   "metadata": {},
   "source": [
    "Cell 5: 對比分析、視覺化與結論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 5: 對比分析、視覺化與結論\n",
    "# 說明：此 Cell 透過圖表和數據，清晰地對比兩個特徵的有效性。\n",
    "# ==============================================================================\n",
    "print(\"--- Cell 5: 開始進行特徵對比分析 ---\")\n",
    "\n",
    "features_to_compare = {\n",
    "    'resp_jaccard': '回應間的字元級 Jaccard 相似度 (您的現有特徵)',\n",
    "    'semantic_similarity': '回應間的語義相似度 (基於 GloVe 的新特徵)'\n",
    "}\n",
    "\n",
    "for key, name in features_to_compare.items():\n",
    "    # 1. 繪製小提琴圖\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.violinplot(x='label', y=key, data=df, cut=0, inner='quartile')\n",
    "    plt.title(f'分析：{name} vs. 最終結果', fontsize=16, pad=20)\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=['A 勝 (label=0)', 'B 勝 (label=1)', '平手 (label=2)'])\n",
    "    plt.xlabel(\"最終結果標籤\", fontsize=12)\n",
    "    plt.ylabel(\"相似度分數\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # 2. 打印統計數據\n",
    "    print(f\"--- 數字統計：{name} ---\")\n",
    "    description = df.groupby('label')[[key]].describe().round(4)\n",
    "    print(description)\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
